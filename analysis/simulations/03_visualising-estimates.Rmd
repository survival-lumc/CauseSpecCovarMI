---
title: "Visualising estimates"
author: "Ed Bonneville"
date: "17-6-2020"
output: 
  html_document:
    toc: yes
    toc_float: 
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  #root.dir = normalizePath(
  #  rprojroot::find_package_root_file()
  #),
  #dev = 'svg', 
  fig.retina = 2, 
  out.width = '95%',
  fig.height = 8,
  echo = FALSE
)

# Load compendium + tidyverse for plotting
devtools::load_all()

library(tidyverse)
library(scales)
library(RColorBrewer)

# Set ggplot theme
theme_set(
  theme_bw(base_size = 14) +
    theme(legend.position = "bottom")
)

palo <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))
#scale_color_manual(values = palo(n))
```


```{r readin}
# Load in processed results
all_estims <- readRDS("all_estims.rds")
#all_estims <- readRDS("analysis/simulations/all_estims.rds")
scenarios <- readRDS("../../inst/testdata/scenarios.rds")
#scenarios <- readRDS("inst/testdata/scenarios.rds")
```

## The scenarios

The table below shows the 119 scenarios that were run. Still to be run are the pilot scenarios with $n = 500$.

```{r scenarios}
scenarios %>% 
  dplyr::select(-seed) %>% 
  knitr::kable()
```


## Plotting estimates

### 1. Estimate variability vs. number of imputed datasets

First, we can plot the average (estimated) model standard error against the number of imputed datasets $m$. 

```{r imps_se}
# https://stackoverflow.com/questions/50960339/create-ggplot2-function-and-specify-arguments-as-variables-in-data-as-per-ggplot
all_estims %>% 
  filter(m <= 50 & m != 0,
         prop_miss == "50%") %>% 
  ggplot(aes(m, se, 
             col = interaction(scen_num, analy)),
             group = interaction(scen_num, analy)) +
  geom_point(size = 2) +
  geom_line(size = 1, alpha = 0.5) +
  xlab("Number of imputed datasets (m)") +
  ylab("Mean model SE") +
  facet_grid(var ~ X_level, scales = "free") + #+
  theme(legend.position = "none")
```

The columns are the levels of $X$, the rows are the variable in question. Each line corresponds to a single scenario. Per facet, that means the combinations of missingness mechanism, strength of mechanisms, proportion missing, baseline hazard shape, imputation method and value of $\beta_1$ (coefficient of $X$ in cause-specific model of event 1, Relapse).

We can also make an equivalent plot using the empirical SEs, which are simply (per scenario) the standard deviation of the 160 replicates of a variable coefficient:

```{r imps_empse}
# https://stackoverflow.com/questions/50960339/create-ggplot2-function-and-specify-arguments-as-variables-in-data-as-per-ggplot
all_estims %>% 
  filter(m <= 50 & m != 0,
         prop_miss == "50%") %>% 
  ggplot(aes(m, emp_se, 
             col = interaction(scen_num, analy)),
             group = interaction(scen_num, analy)) +
  geom_point(size = 2) +
  geom_line(size = 1, alpha = 0.5) +
  facet_grid(var ~ X_level, scales = "free") + 
  xlab("Number of imputed datasets (m)") +
  ylab("Empirical SE") +
  theme(legend.position = "none")
```

Henceforth in the document, we only show the results for the $m = 50$ case.

### 2. Basic estimates plot 

We can start with a basic, facetted estimates plot. This will cover the following four scenarios:

```{r onescen_tab}
scenarios %>% filter(
   eta1 == "-2" | is.na(eta1),
    haz_shape == "similar",
    beta1 == "1",
    n == 2000,
    X_level == "continous",
    prop_miss == "0.5"
) %>% 
  knitr::kable()
```

This time, the columns are the variables and the rows are the missingness mechanisms. The y axis plots estimated - true, and the 95\% confidence intervals are built symmetrically based on the empirical SEs.

```{r onescen}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    n == 2000,
    eta1 != "Weak",
    haz_shape == "similar",
    beta1 == "1",
    X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, bias, col = analy)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  geom_segment(
    aes(
      y = bias - qnorm(0.975) * emp_se,
      yend = bias + qnorm(0.975) * emp_se,
      x = analy,
      xend = analy
    ), 
    size = 2,
    alpha = .5
  ) + 
  geom_point(size = 2) +
  xlab("Methods") +
  ylab("Estimate - True (95% Emp SE-based CI)") +
  geom_hline(aes(yintercept = 0), linetype = "dashed")  +
  facet_grid(miss_mech ~ var) +
  scale_color_brewer(palette="Dark2") +
  theme(legend.position = "none")
```

The above could be extended by adding the level of $X$ to the facets, doubling the amount of scenarios shown (at the cost of the plot becoming very cramped due to the scale of y axis):

```{r onescen_ext}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    eta1 != "Weak",
    n == 2000,
    haz_shape == "similar",
    beta1 == "1",
    #X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, bias, col = analy)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  geom_segment(
    aes(
      y = bias - qnorm(0.975) * emp_se,
      yend = bias + qnorm(0.975) * emp_se,
      x = analy,
      xend = analy
    ), 
    size = 2,
    alpha = .5
  ) + 
  geom_point(size = 2) +
  xlab("Methods") +
  ylab("Estimate - True (95% Emp SE-based CI)") +
  geom_hline(aes(yintercept = 0), linetype = "dashed")  +
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  theme(legend.position = "none")
```


### 3. Using 'grouped lolly plots' to look at bias

Given that bias is the main measure of interest, 'grouped lolly plots' can be used. We can keep the previous facets, and add another manipulation of interest like the strength of the mechanism. Additionally, incorporation of the monte-carlo SE is also straightforward.

We we will be able to capture the following 12 scenarios:

```{r lolly_tab}
scenarios %>% filter(
    !is.na(eta1),
    haz_shape == "similar",
    beta1 == "1",
    n == 2000,
    #X_level == "continous",
    prop_miss == "0.5") %>% 
  knitr::kable()
```

In the plot below,

* Each colour corresponds to a single method
* There are two linetypes/shapes per method, to distinguish between a weak and strong missingness mechanisms
* The brackets around the points correspond to 95\% confidence intervals build symmetrically based on monte-carlo SEs


```{r lolly_eta1}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    haz_shape == "similar",
    miss_mech != "MCAR",
    n == 2000,
    beta1 == "1",
    #X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, bias, col = analy, 
             group = eta1, shape = eta1,
             linetype = eta1)) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(position = position_dodge(0.75), size = 2) + 
  geom_linerange(aes(ymin = 0, ymax = bias, 
                     xmin = analy, xmax = analy),
                 position = position_dodge(0.75)) +
  coord_flip() + 
  geom_point(aes(x = analy, y = bias + qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 41,
             size = 1.5) + 
  geom_point(aes(x = analy, y = bias - qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 40, 
             size = 1.5) + 
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  xlab("Methods") +
  ylab("Bias (Monte-Carlo SE)") +
  guides(colour = FALSE)
```


We can make the same plot but for another manipulation, such as the shape of the baseline hazards.

In this case, we actually manage to fully capture 16 scenarios:

```{r lolly_hazshape_tab}
scenarios %>% filter(
    is.na(eta1) | eta1 == "-2",
    #haz_shape == "similar",
    beta1 == "1",
    n == 2000,
    #X_level == "continous",
    prop_miss == "0.5") %>% 
  knitr::kable()
```


```{r lolly_hazshape}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    #haz_shape == "similar",
    #miss_mech != "MCAR",
    beta1 == "1",
    n == 2000,
    eta1 == "Strong" | is.na(eta1),
    #X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, bias, col = analy, 
             group = haz_shape, shape = haz_shape,
             linetype = haz_shape)) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(position = position_dodge(0.75), size = 2) + 
  geom_linerange(aes(ymin = 0, ymax = bias, 
                     xmin = analy, xmax = analy),
                 position = position_dodge(0.75)) +
  coord_flip() + 
  geom_point(aes(x = analy, y = bias + qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 41,
             size = 1.5) + 
  geom_point(aes(x = analy, y = bias - qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 40, 
             size = 1.5) + 
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  xlab("Methods") +
  ylab("Bias (Monte-Carlo SE)") +
  guides(colour = FALSE)

```

### 4. A (grouped) lolly plot for coverage

We can make an equivalent lolly plot for coverage (and associated monte-carlo SEs), where the reference in now a nominal 95\% level. Of course, these values have to be interpreted in tandem with the bias plots, as severe bias will also lead to sever under-coverage.

In the case below, we choose to also include proportion of missing values in the plot:

```{r lolly_cover}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    haz_shape == "similar",
    #miss_mech != "MCAR",
    beta1 == "1",
    eta1 == "Strong",
    n == 2000
    #X_level == "continuous",
    #prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, cover, col = analy, 
             group = prop_miss, shape = prop_miss,
             linetype = prop_miss)) +
  geom_hline(yintercept = 0.95, linetype = "dashed") + 
  geom_point(position = position_dodge(0.75), size = 2) + 
  geom_linerange(aes(ymin = 0.95, ymax = cover, 
                     xmin = analy, xmax = analy),
                 position = position_dodge(0.75)) +
  coord_flip() + 
  geom_point(aes(x = analy, y = cover + qnorm(0.975) * mcarlo_se_cover),
             position = position_dodge(0.75), shape = 41,
             size = 1.5) + 
  geom_point(aes(x = analy, y = cover - qnorm(0.975) * mcarlo_se_cover),
             position = position_dodge(0.75), shape = 40, 
             size = 1.5) + 
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  guides(colour = FALSE) +
  xlab("Methods") +
  ylab("Coverage (Monte-Carlo SE)") + 
  ggtitle("Coverage with haz_shape = similar, eta1 = strong")
```


### 5. Nested loop plots - single

Main reference is [this paper](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-129) : nested loop plots (NLPs) can be really handy when we have a lot of scenarios to summarise.

I have made a variant of it which (I think) works rather well for this study.

The following 16 scenarios will be shown:

```{r nlp_tab}
scenarios %>% filter(
    miss_mech == "MAR",
    X_level == "continous",
    n == 2000,
    beta1 != "0") %>% 
  knitr::kable()
```

Here is the plot (*For variable X.1 only*):

```{r nested_loop_X1}

# Attempt at nested loop plot for X1
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "continuous"
] %>% 
  
  # Add relative bias 
  .[, rel_bias := 100 * bias / true]


ggplot_nlp(dat = dat_nlp,
           estim = "rel_bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = NULL) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias")

  #ylab("Estimate (X.1)") +
  #ggtitle("Nested loop plot for  MAR and contin X")  
#what am i looking at

# Make function! and look at below!
#https://journals.sagepub.com/doi/full/10.1177/0962280218780856
#https://github.com/matherealize/loopR

```

Scenarios are ordered along the x axis based on some manipulated variables (eta1, haz shape and prop_missing). From left to right you are looking at clusters of 6 points, corresponding to the six methods used. The scenario is grouped using point colour, and shape is use to tell the methods apart.

In the plot below (for example) we can identify a scenario as follows:

```{r}
p <- ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = NULL) + 
  annotate("text", label = "Scenario 52 \n (50%, diff, strong)",
           x = 7.5, y = -0.15,
           size = 3, hjust = 0.5, angle = 90) +
  #scale_color_brewer(palette="Dark2") +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias")

p$layers <- c(
  geom_rect(aes(xmin = 7, xmax = 8, ymin = -0.4, ymax = 0.1),
            alpha = 0.1, fill = "grey70", col = NA),
  p$layers
)
p
```

Just by looking across the plot, you can already make some tentative statements (which of course will need further investigation). In this case, for the estimated $\hat{\beta_1}$ with missingness at random and continous $X$:

* Bias is present almost exclusively for the MICE-based methods ($CH_{1}$, $CH_{12}$ and $CH_{12_\text{int}}$). CCA, and smcfcs mostly unbiased.

* Bias is greater when $\beta_1 = 1$ compared to $\beta_1 = 0.5$ (look at left half of the plot vs right half).

* Bias is greater when proportion missingness is higher (compare the halves *within* each half of the plot).

* Bias seems greater when the baseline hazard shapes are very different.

* A stronger MAR seems to improve performance of the MICE-based methods.


### 5. Nested loop plots - facetted

If we go one step further and use facets, we can display a whopping **96 scenarios** in one plot!! Of course this plot should only be use as a general impression, but is still super useful.

Given how cramped it looks on a vertical html, this would be plotted full page on an A4 in landscape.

```{r nlp_one}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech != "MCAR" #&
    #X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = NULL) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") +
  facet_grid(X_level ~ miss_mech) 
```



## NLPs - in tabs (still X.1) {.tabset}

We can use the tab functionality in Rmarkdown to cycle through these plots.

### MAR (X contin.)


```{r NLP_MAR_contin, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR (X .binary)


```{r NLP_MAR_binary, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MNAR (X .contin)


```{r NLP_MNAR_contin, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MNAR" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MNAR (X binary)


```{r NLP_MNAR_binary, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MNAR" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR_GEN (X contin)


```{r NLP_MARG_contin, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MAR_GEN" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR_GEN (X binary)


```{r NLP_MARG_binary, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.1" &
    beta1 != "0" &
    miss_mech == "MAR_GEN" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```





## NLPs - X.2 {.tabset}

We do the same, but now showing the results for **X.2**

### MAR (X contin.)


```{r NLP_MAR_contin_X2, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.2" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR (X .binary)


```{r NLP_MAR_binary_X2, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.2" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MNAR (X .contin)


```{r NLP_MNAR_contin_X2, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.2" &
    beta1 != "0" &
    miss_mech == "MNAR" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MNAR (X binary)


```{r NLP_MNAR_binary_X2, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.2" &
    beta1 != "0" &
    miss_mech == "MNAR" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR_GEN (X contin)


```{r NLP_MARG_contin_X2, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.2" &
    beta1 != "0" &
    miss_mech == "MAR_GEN" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.3) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR_GEN (X binary)


```{r NLP_MARG_binary_X2, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "X.2" &
    beta1 != "0" &
    miss_mech == "MAR_GEN" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```



## NLPs - Z.1 {.tabset}

We do the same, but now showing the results for **Z.1**. We then omit the results for Z.2, as they are very similar (little bias)

### MAR (X contin.)


```{r NLP_MAR_contin_Z1, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "Z.1" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR (X .binary)


```{r NLP_MAR_binary_Z1, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "Z.1" &
    beta1 != "0" &
    miss_mech == "MAR" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MNAR (X .contin)


```{r NLP_MNAR_contin_Z1, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "Z.1" &
    beta1 != "0" &
    miss_mech == "MNAR" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MNAR (X binary)


```{r NLP_MNAR_binary_Z1, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "Z.1" &
    beta1 != "0" &
    miss_mech == "MNAR" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR_GEN (X contin)


```{r NLP_MARG_contin_Z1, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "Z.1" &
    beta1 != "0" &
    miss_mech == "MAR_GEN" &
    X_level == "continuous"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.3) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```


### MAR_GEN (X binary)


```{r NLP_MARG_binary_Z1, fig.height=6}
dat_nlp <- all_estims[
  m %in% c(0, 50) & 
    n == 2000 &
    var == "Z.1" &
    beta1 != "0" &
    miss_mech == "MAR_GEN" &
    X_level == "binary"
]

ggplot_nlp(dat = dat_nlp,
           estim = "bias", 
           method_var = "analy", 
           true = 0, 
           step_factors = c("beta1", "prop_miss", "haz_shape", "eta1"),
           point_dodge = 1,
           text_size = 3, 
           pointsize = 1.5, 
           step_labels = NULL,
           top_step = -0.25) +
  ggplot2::guides(shape = guide_legend("Method"),
                  linetype = guide_legend("Method")) +
  ylab("Bias") 
```
