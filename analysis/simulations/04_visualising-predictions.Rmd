---
title: "Visualising predictions"
author: "Ed Bonneville"
date: "22-4-2020"
output: 
  html_document:
    toc: yes
    toc_float: 
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  #root.dir = normalizePath(
  #  rprojroot::find_package_root_file()
  #),
  #dev = 'svg', 
  fig.retina = 2, 
  out.width = '95%',
  fig.height = 8,
  echo = FALSE
)

# Load compendium + tidyverse for plotting
devtools::load_all()

library(ggplot2)
library(scales)
library(RColorBrewer)
library(ggridges)
library(ggChernoff)

# Set ggplot theme
theme_set(
  theme_bw(base_size = 14) +
    theme(legend.position = "bottom")
)

palo <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))
```


```{r readin}
# Load in processed results
all_preds <- readRDS("all_preds.rds")
#all_preds <- readRDS("analysis/simulations/all_preds.rds")

scenarios <- readRDS("../../inst/testdata/scenarios.rds")
#scenarios <- readRDS("inst/testdata/scenarios.rds")
```


## The scenarios

For completeness, we again show the 119 scenarios that were run:

```{r scenarios}
scenarios %>% 
  dplyr::select(-seed) %>% 
  knitr::kable()
```


## Refresher on set-up


### Data set-up


In addition to manipulating data-generating mechanisms, in the prediction part of the study we look at predicted vs. true state probabilities 6 months, 5 years and 10 years after baseline, for event-free survival (EFS), relapse (REL) and non-relapse mortality (NRM). These probabilities are predicted for a set of reference patients (combo-X_Z). Here is an example subset of the results:

```{r refresher}
all_preds[order(analy, `combo-X_Z`)] %>% 
  .[, .SD[1:9]] %>% 
  .[, .(analy, `combo-X_Z`, times, state, prob, true)] %>% 
  knitr::kable()
```

These are the reference patients:

```{r refpats, fig.height=4}
make_covar_grid(cbind.data.frame(X = as.factor(c(0, 1)),
                                 Z = c(0, 1))) %>% 
  ggplot(aes(factor(val_X), val_Z)) +
  geom_chernoff(size = 5, fill = 'steelblue1') +
  xlab("X (binary)") +
  ylab("Z") +
  ggtitle("Reference patients (binary X)")

make_covar_grid(cbind.data.frame(X =c(0, 1),
                                 Z = c(0, 1))) %>% 
  ggplot(aes(val_X, val_Z)) +
  geom_chernoff(size = 5, fill = 'steelblue1') +
  xlab("X (continous)") +
  ylab("Z") +
  ggtitle("Reference patients (continuous X)")

```


### What probabilities are we predicting?

Recall (will be in sim doc shortly) that the data is generated with the following parameters:

$$
\begin{align*}
&[\alpha_1, h_{0,1}, \beta_1, \gamma_1 = 1], \\
&[\alpha_2 = 0.53, h_{0,2} = 0.21, \beta_2 = 0.5, \gamma_2=0.5 \ ],	
\end{align*}
$$

with $(\alpha_1,h_{1,0}) \in \{(0.58,0.19),(1.5,0.04)\}$ (respectively the 'similar' and 'different' baseline hazard shape conditions), and $\beta_1 \in \{0.5,1\}$.

Given these set(s) of baseline parameters, `msfit()` is used and baseline hazards are computed. Afterwards, these are multiplied by a linear predictor
(determined by the estimated cox coefficients, and the reference patient we wish to predict for), and thereafter used to compute state probabilities.

For continuous $X$, we can get a feeling for what the true probabilities look like in the plot below. The vertical facets represent the combination of $\beta_1$ and $\alpha_1$, and the horizontal facets represent the reference patients. For example, 'mean_X-Z_mean' are the baseline true probabilities (when $X = 0$ and $Z = 0$). 

```{r checks}
all_preds %>% 
  # Subset interest
  .[m %in% c(0, 50) & 
            #beta1 == "0.5" &
            #haz_shape == "similar" &
            X_level == "continuous", 
    .(`combo-X_Z`, true, state, times, beta1, haz_shape)] %>%
  ggplot(aes(times, true, col = state, group = state)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2),
                   labels = c("6 mo.", "5Y", "10Y")) +
  geom_point(size = 2) +
  geom_line() +
  ylab("True state probability") +
  xlab("Prediction horizons") +
  ylim(c(0, 1)) +
  facet_grid(`combo-X_Z`~ haz_shape * beta1, scales = "free") +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "top")
```

The same plot can be made with binary $X$, where there is one extra reference patient.


## Plotting predictions


### 1. Primer: bias is (overall) small


Before proceeding with interpretation of the upcoming prediction results, **mind the scale** : the size of the bias across all conditions seems to be rather small (below 5\% absolute bias). 

We can illustrate this in the plots below, which plots the overall distribution of bias (predicted - true probabilities) for all states, by analysis, reference patient and missingness mechanism. A plot is made for both binary $X$ and continuous $X$.


```{r bias, message=FALSE}
all_preds[X_level == "continuous"] %>%
  ggplot(aes(x = bias, y = analy, fill = analy)) +
  geom_density_ridges(alpha = 0.5, na.rm = T) +
  scale_x_continuous("Bias", guide = guide_axis(n.dodge = 2),
                     limits = c(-0.05, 0.05)) +
  facet_grid(`combo-X_Z` ~ miss_mech) +
  theme(legend.position = "none") +
  ggtitle("Conitinuous X") 


all_preds[X_level == "binary"] %>% 
  ggplot(aes(x = bias, y = analy, fill = analy)) +
  geom_density_ridges(alpha = 0.5, na.rm = T) +
  scale_x_continuous("Bias", guide = guide_axis(n.dodge = 2),
                     limits = c(-0.05, 0.05)) +
  facet_grid(`combo-X_Z` ~ miss_mech) +
  theme(legend.position = "none") +
  ggtitle("Binary X")
```

The nested loop plots (NLP) later on will help in breaking this down.

### 2. Empirical SE of predictions vs. number of imputed datasets

Like in the estimates html, we can plot the empirical standard error of the predicted probabilities against the number of imputed datasets $m$. The individual lines have the same interpretation: a single scenario based on the combinations of missingness mechanism, strength of mechanisms, proportion missing, baseline hazard shape, imputation method and value of $\beta_1$ (coefficient of $X$ in cause-specific model of event 1, Relapse).

Here also we facet by prediction horizon with level of $X$, and state.


```{r empSE_m}
all_preds[m <= 50 & 
            m != 0 &
            prop_miss == "50%" &
            `combo-X_Z` %in% c("mean_X-Z_mean",
                               "0_X-Z_mean")] %>% 
  ggplot(aes(m, emp_se, 
             col = interaction(scen_num, analy)),
             group = interaction(scen_num, analy)) +
  geom_point(size = 2) +
  scale_x_continuous("Number of imputed datasets (m)", 
                     guide = guide_axis(n.dodge = 2)) +
  geom_line(size = 1, alpha = 0.5) +
  facet_grid(state ~ X_level * times, scales = "free") + 
  #xlab("Number of imputed datasets (m)") +
  ylab("Empirical SE predicted probabilities") +
  theme(legend.position = "none")
```


Some points:

- Probabilities are more variable the further the prediction horizon.
- The reduction in variability with greater $m$ is more pronounced in the binary than in the continuous case.
- The same plot will be made with the $n = 500$ simulations, where we expect the decrease to be more noticeable.


### 3. Prediction bias using lolly plots (baseline hazards)

Similarly to the estimate, we can make grouped lolly plots to illutrate bias. Let us focus on the following scenarios, looking only at the baseline case (reference patient $X = 0$ and $Z = 0$).

```{r scen_lolly1}
scenarios %>% 
  data.table() %>% 
  .[beta1 == 1 &
            eta1 %in% c(-1, NA) &
            haz_shape == "similar" &
            X_level == "continous"& 
            prop_miss == 0.5] %>% 
  knitr::kable()
```

The grouping in the plot is the state:

```{r test_lolly}
all_preds[m %in% c(0, 50) & 
            beta1 == "1" &
            eta1 %in% c("Weak", "None") &
            haz_shape == "similar" &
            `combo-X_Z` == "mean_X-Z_mean" &
            X_level == "continuous"& 
            prop_miss == "50%"] %>%
  ggplot_lolly(dat = .,
               estim = "bias",
               method_var = "analy", 
               group = "state",
               true = 0,
               mcarlo_se = "mcarlo_se_bias",
               facets = c("miss_mech", "times")) +
  scale_color_brewer(palette="Dark2") +
  xlab("Analysis") +
  ylab("Bias (Predicted - True)") +
  ggtitle("For combo mean_X-Z_mean")
```


Note the large biases for EFS: we are only predicting REL and NRM, so EFS prediction pays the compound price of bias in both/either REL and NRM probabilities. Given this, we could exclude EFS from remaining figures.

If we focus (same scenario) on NRM, we can also illustrate the empirical standard errors as follows:

```{r ests}
all_preds[m %in% c(0, 50) & 
            beta1 == "1" &
            eta1 %in% c("Weak", "None") &
            haz_shape == "similar" &
            `combo-X_Z` == "-1SD_X-Z_-1SD" &
            state == "NRM" &
            #X_level == "continuous"& 
            prop_miss == "50%"] %>%
  ggplot_estimates(dat = ., 
                   estim = "bias", 
                   method_var = "analy", 
                   se = "emp_se", 
                   true = 0,
                   facets = c("miss_mech", "times")) +
  scale_color_brewer(palette="Dark2") +
  xlab("Analysis") +
  ylab("Bias (Predicted - True)") +
  ggtitle("NRM")
```

It also shows an important point: the efficiency of multiple imputation is clear also for predicted probabilities, particularly with later prediction horizons.


```{r time_x, echo=FALSE, eval=FALSE}
# Over time.. (not show)
all_preds %>% 
  dplyr::filter(
    m %in% c(0, 50),
    beta1 == "1",
    eta1 %in% c("Weak", "None"),
    haz_shape == "similar",
    `combo-X_Z` == "-1SD_X-Z_-1SD",
    X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(x = times, y = prob, col = analy, group = analy)) +
  geom_point(size = 2) +
  geom_line(alpha = .5) +
  #scale_x_continuous(guide = guide_axis(n.dodge = 2)) +
  facet_grid(miss_mech ~ state) +
  ylim(c(0, 1)) +
  scale_color_brewer(palette="Dark2") 

# Can also just plot rmse instead

```


### 4. Lolly plot for  RMSE

We can also make a lolly plot using RMSE (using same scenarios as preceding section). Note that the monte carlo SEs are omitted from the plot, since for RMSE they need to be computed using the [jack-knife technique](https://cran.r-project.org/web/packages/simhelpers/vignettes/MCSE.html), which is not yet implemented in this study. We also omit EFS from the plot:

```{r lolly_RMSE}
all_preds[m %in% c(0, 50) & 
            beta1 == "1" &
            eta1 %in% c("Weak", "None") &
            haz_shape == "similar" &
            state != "EFS" &
            `combo-X_Z` == "mean_X-Z_mean" &
            X_level == "continuous"& 
            prop_miss == "50%"] %>%
  ggplot_lolly(dat = .,
               estim = "rmse",
               method_var = "analy", 
               group = "state",
               true = 0,
               #mcarlo_se = "mcarlo_se_bias",
               facets = c("miss_mech", "times")) +
  scale_color_brewer(palette="Dark2") +
  xlab("Analysis") +
  ylab("RMSE")
```


### 5. NLP (Baseline)

We start simple by looking only at scenarios high missingness proportion and $\eta_1 \in \{ 0, -1\}$ (i.e. MCAR and 'weak' MAR, MNAR and MAR_GEN). 

The left column represents the baseline case for scenarios with binary $X$ and analogously the right column for continous $X$. 

```{r NLP_miss}
all_preds[m %in% c(0, 50) & 
            state != "EFS" &
            beta1 != "0" &
            eta1 %in% c("None", "Weak") &
           # miss_mech != "MCAR" & 
           # times == "10 years" &  
            #haz_shape == "similar" &
            `combo-X_Z` %in% c("mean_X-Z_mean",
                               "0_X-Z_mean") & 
            #X_level == "continuous"& 
            prop_miss == "50%"
          ] %>% 
  ggplot_nlp(dat = .,
             estim = "bias", 
             method_var = "analy", 
             true = 0, 
             step_factors = c("state", "times", "beta1",
                              "haz_shape"),# "eta1"), 
             text_size = 2, 
             pointsize = 1,
             top_step = -0.1,
             height_betw_steps = 0.025,
             height_steps = 0.025) +
  facet_grid(miss_mech ~ `combo-X_Z`) +
  ylab("Bias")
```


### 6. NLP - with covariate effects

For the following, we assume we are interested primarily in a **10 year prediction** of REL and NRM probabilities. We focus on scenarios with high MAR, MAR_GEN and MNAR missingness.

### 6.1. NLP - protective

Let us look at reference patients '1_X-Z_-1SD' and '-1SD_X-Z_-1SD':

```{r NLP_prot, fig.height = 11}
all_preds[m %in% c(0, 50) & 
            state != "EFS" &
            beta1 != "0" &
            #eta1 %in% c("Weak") &
            miss_mech != "MCAR" & 
            times == "10 years" &  
            #haz_shape == "similar" &
            `combo-X_Z` %in% c("1_X-Z_-1SD",
                               "-1SD_X-Z_-1SD") & 
            #X_level == "continuous"& 
            prop_miss == "50%"
          ] %>% 
  ggplot_nlp(dat = .,
             estim = "bias", 
             method_var = "analy", 
             true = 0, 
             step_factors = c("state", "beta1",
                              "haz_shape", "eta1"),# "eta1"), 
             text_size = 3, 
             pointsize = 1,
             top_step = -0.125,
             height_betw_steps = 0.025,
             height_steps = 0.015) +
  facet_grid(miss_mech ~ `combo-X_Z`) +
  ylab("Bias")
```

There are clear problems with scale, so let us zoom in on the MAR case:

```{r NLP_prot_zoom, fig.height = 11}
all_preds[m %in% c(0, 50) & 
            state != "EFS" &
            beta1 != "0" &
            #eta1 %in% c("Weak") &
            miss_mech == "MAR" & 
            times == "10 years" &  
            #haz_shape == "similar" &
            `combo-X_Z` %in% c("1_X-Z_-1SD",
                               "-1SD_X-Z_-1SD") & 
            #X_level == "continuous"& 
            prop_miss == "50%"
          ] %>% 
  ggplot_nlp(dat = .,
             estim = "bias", 
             method_var = "analy", 
             true = 0, 
             step_factors = c("state", "beta1",
                              "haz_shape", "eta1"),# "eta1"), 
             text_size = 3, 
             pointsize = 1,
             top_step = -0.02,
             height_betw_steps = 0.0075,
             height_steps = 0.005
             ) +
  facet_grid(`combo-X_Z` ~ .) +
  ylab("Bias") +
  ggtitle("MAR")
```


### 6.2. NLP - hazardous


We focus on reference patients '1_X-Z_+1SD' and '+1SD_X-Z_+1SD':

```{r NLP_haz, fig.height = 11}
all_preds[m %in% c(0, 50) & 
            state != "EFS" &
            beta1 != "0" &
            #eta1 %in% c("Weak") &
            miss_mech != "MCAR" & 
            times == "10 years" &  
            #haz_shape == "similar" &
            `combo-X_Z` %in% c("1_X-Z_+1SD",
                               "+1SD_X-Z_+1SD") & 
            #X_level == "continuous"& 
            prop_miss == "50%"
          ] %>% 
  ggplot_nlp(dat = .,
             estim = "bias", 
             method_var = "analy", 
             true = 0, 
             step_factors = c("state", "beta1",
                              "haz_shape", "eta1"),# "eta1"), 
             text_size = 3, 
             pointsize = 1,
             top_step = -0.2,
             height_betw_steps = 0.025,
             height_steps = 0.015) +
  facet_grid(miss_mech ~ `combo-X_Z`) +
  ylab("Bias")
```

Again we have some scale problems, let's zoom in on the MAR case:

```{r NLP_haz_zoom, fig.height = 11}
all_preds[m %in% c(0, 50) & 
            state != "EFS" &
            beta1 != "0" &
            #eta1 %in% c("Weak") &
            miss_mech == "MAR" & 
            times == "10 years" &  
            #haz_shape == "similar" &
            `combo-X_Z` %in% c("1_X-Z_+1SD",
                               "+1SD_X-Z_+1SD") & 
            #X_level == "continuous"& 
            prop_miss == "50%"
          ] %>% 
  ggplot_nlp(dat = .,
             estim = "bias", 
             method_var = "analy", 
             true = 0, 
             step_factors = c("state", "beta1",
                              "haz_shape", "eta1"),# "eta1"), 
             text_size = 3, 
             pointsize = 1,
             top_step = -0.015,
             height_betw_steps = 0.0025
             #height_steps = 0.005
             ) +
  facet_grid(`combo-X_Z` ~ .) +
  ylab("Bias") +
  ggtitle("MAR")
```

