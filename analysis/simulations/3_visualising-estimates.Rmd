---
title: "Visualising estimates"
author: "Ed Bonneville"
date: "16-4-2020"
output: 
  html_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  #root.dir = normalizePath(
  #  rprojroot::find_package_root_file()
  #),
  #dev = 'svg', 
  fig.retina = 2, 
  out.width = '95%',
  fig.height = 8,
  echo = FALSE
)

# Load compendium + tidyverse for plotting
devtools::load_all()

library(tidyverse)
library(scales)
library(RColorBrewer)

# Set ggplot theme
theme_set(
  theme_bw(base_size = 14) +
    theme(legend.position = "bottom")
)
```


```{r readin}
# Load in processed results
all_estims <- readRDS("all_estims.rds")
#all_estims <- readRDS("analysis/simulations/all_estims.rds")
scenarios <- readRDS("../../inst/testdata/scenarios.rds")
#scenarios <- readRDS("inst/testdata/scenarios.rds")
```

## The scenarios

The table below shows the 119 scenarios that were run. Still to be run are the pilot scenarios with $n = 500$.

```{r scenarios}
scenarios %>% 
  dplyr::select(-seed) %>% 
  knitr::kable()
```


## Plotting estimates

### 1. Estimate variability vs. number of imputated datasets

First, we can plot the average (estimated) model standard error against the number of imputed datasets $m$. 

```{r imps_se}
# https://stackoverflow.com/questions/50960339/create-ggplot2-function-and-specify-arguments-as-variables-in-data-as-per-ggplot
all_estims %>% 
  filter(m <= 50 & m != 0,
         prop_miss == "50%") %>% 
  ggplot(aes(m, se, 
             col = interaction(scen_num, analy)),
             group = interaction(scen_num, analy)) +
  geom_point(size = 2) +
  geom_line(size = 1, alpha = 0.5) +
  xlab("Number of imputed datasets (m)") +
  ylab("Mean model SE") +
  facet_grid(var ~ X_level, scales = "free") + #+
  theme(legend.position = "none")
```

The columns are the levels of $X$, the rows are the variable in question. Each line corresponds to a single scenario. Per facet, that means the combinations of missingness mechanism, strength of mechanisms, proportion missing, baseline hazard shape, imputation method and value of $\beta_1$ (coefficient of $X$ in cause-specific model of event 1, Relapse).

We can also make an equivalent plot using the empirical SEs, which are simply (per scenario) the standard deviation of the 160 replicates of a variable coefficient:

```{r imps_empse}
# https://stackoverflow.com/questions/50960339/create-ggplot2-function-and-specify-arguments-as-variables-in-data-as-per-ggplot
all_estims %>% 
  filter(m <= 50 & m != 0,
         prop_miss == "50%") %>% 
  ggplot(aes(m, emp_se, 
             col = interaction(scen_num, analy)),
             group = interaction(scen_num, analy)) +
  geom_point(size = 2) +
  geom_line(size = 1, alpha = 0.5) +
  facet_grid(var ~ X_level, scales = "free") + 
  xlab("Number of imputed datasets (m)") +
  ylab("Empirical SE") +
  theme(legend.position = "none")
```

Henceforth in the document, we only show the results for the $m = 50$ case.

### 2. Basic estimates plot 

We can start with a basic, facetted estimates plot. This will cover the following four scenarios:

```{r onescen_tab}
scenarios %>% filter(
   eta1 == "-2" | is.na(eta1),
    haz_shape == "similar",
    beta1 == "1",
    X_level == "continous",
    prop_miss == "0.5"
) %>% 
  knitr::kable()
```

This time, the columns are the variables and the rows are the missingness mechanisms. The dashed lines in the facets are the true (data-generating)
coefficient value, and the 95\% confidence intervals are built symmetrically based on the empirical SEs.

```{r onescen}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    eta1 != "Weak",
    haz_shape == "similar",
    beta1 == "1",
    X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, est, col = analy)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  geom_segment(
    aes(
      y = est - qnorm(0.975) * emp_se,
      yend = est + qnorm(0.975) * emp_se,
      x = analy,
      xend = analy
    ), 
    size = 2,
    alpha = .5
  ) + 
  geom_point(size = 2) +
  xlab("Methods") +
  ylab("Estimate (95% Emp SE-based CI)") +
  geom_hline(aes(yintercept = true), linetype = "dashed")  +
  facet_grid(miss_mech ~ var) +
  scale_color_brewer(palette="Dark2") +
  theme(legend.position = "none")
```

The above could be extended by adding the level of $X$ to the facets, doubling the amount of scenarios shown (at the cost of the plot becoming very cramped due to the scale of y axis):

```{r onescen_ext}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    eta1 != "Weak",
    haz_shape == "similar",
    beta1 == "1",
    #X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, est, col = analy)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  geom_segment(
    aes(
      y = est - qnorm(0.975) * emp_se,
      yend = est + qnorm(0.975) * emp_se,
      x = analy,
      xend = analy
    ), 
    size = 2,
    alpha = .5
  ) + 
  geom_point(size = 2) +
  xlab("Methods") +
  ylab("Estimate (95% Emp SE-based CI)") +
  geom_hline(aes(yintercept = true), linetype = "dashed")  +
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  theme(legend.position = "none")
```


### 3. Using 'grouped lolly plots' to look at bias

Given that bias is the main measure of interest, 'grouped lolly plots' can be used. We can keep the previous facets, and add another manipulation of interest like the strength of the mechanism. Additionally, incorporation of the monte-carlo SE is also straightforward.

We we will be able to capture the following 12 scenarios:

```{r lolly_tab}
scenarios %>% filter(
    !is.na(eta1),
    haz_shape == "similar",
    beta1 == "1",
    #X_level == "continous",
    prop_miss == "0.5") %>% 
  knitr::kable()
```

In the plot below,

* Each colour corresponds to a single method
* There are two linetypes/shapes per method, to distinguish between a weak and strong missingness mechanisms
* The brackets around the points correspond to 95\% confidence intervals build symmetrically based on monte-carlo SEs


```{r lolly_eta1}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    haz_shape == "similar",
    miss_mech != "MCAR",
    beta1 == "1",
    #X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, bias, col = analy, 
             group = eta1, shape = eta1,
             linetype = eta1)) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(position = position_dodge(0.75), size = 2) + 
  geom_linerange(aes(ymin = 0, ymax = bias, 
                     xmin = analy, xmax = analy),
                 position = position_dodge(0.75)) +
  coord_flip() + 
  geom_point(aes(x = analy, y = bias + qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 41,
             size = 1.5) + 
  geom_point(aes(x = analy, y = bias - qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 40, 
             size = 1.5) + 
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  xlab("Methods") +
  ylab("Bias (Monte-Carlo SE)") +
  guides(colour = FALSE)
```


We can make the same plot but for another manipulation, such as the shape of the baseline hazards.

In this case, we actually manage to fully capture 16 scenarios:

```{r lolly_hazshape_tab}
scenarios %>% filter(
    is.na(eta1) | eta1 == "-2",
    #haz_shape == "similar",
    beta1 == "1",
    #X_level == "continous",
    prop_miss == "0.5") %>% 
  knitr::kable()
```


```{r lolly_hazshape}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    #haz_shape == "similar",
    #miss_mech != "MCAR",
    beta1 == "1",
    eta1 == "Strong" | is.na(eta1),
    #X_level == "continuous",
    prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, bias, col = analy, 
             group = haz_shape, shape = haz_shape,
             linetype = haz_shape)) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(position = position_dodge(0.75), size = 2) + 
  geom_linerange(aes(ymin = 0, ymax = bias, 
                     xmin = analy, xmax = analy),
                 position = position_dodge(0.75)) +
  coord_flip() + 
  geom_point(aes(x = analy, y = bias + qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 41,
             size = 1.5) + 
  geom_point(aes(x = analy, y = bias - qnorm(0.975) * mcarlo_se_bias),
             position = position_dodge(0.75), shape = 40, 
             size = 1.5) + 
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  xlab("Methods") +
  ylab("Bias (Monte-Carlo SE)") +
  guides(colour = FALSE)

```

### 4. A (grouped) lolly plot for coverage

We can make an equivalent lolly plot for coverage (and associated monte-carlo SEs), where the reference in now a nominal 95\% level. Of course, these values have to be interpreted in tandem with the bias plots, as severe bias will also lead to sever under-coverage.

In the case below, we choose to also include proportion of missing values in the plot:

```{r lolly_cover}
all_estims %>% 
  filter(
    m %in% c(0, 50),
    haz_shape == "similar",
    #miss_mech != "MCAR",
    beta1 == "1",
    eta1 == "Strong",
    #X_level == "continuous",
    #prop_miss == "50%"
  ) %>% 
  ggplot(aes(analy, cover, col = analy, 
             group = prop_miss, shape = prop_miss,
             linetype = prop_miss)) +
  geom_hline(yintercept = 0.95, linetype = "dashed") + 
  geom_point(position = position_dodge(0.75), size = 2) + 
  geom_linerange(aes(ymin = 0.95, ymax = cover, 
                     xmin = analy, xmax = analy),
                 position = position_dodge(0.75)) +
  coord_flip() + 
  geom_point(aes(x = analy, y = cover + qnorm(0.975) * mcarlo_se_cover),
             position = position_dodge(0.75), shape = 41,
             size = 1.5) + 
  geom_point(aes(x = analy, y = cover - qnorm(0.975) * mcarlo_se_cover),
             position = position_dodge(0.75), shape = 40, 
             size = 1.5) + 
  facet_grid(miss_mech * X_level ~ var) +
  scale_color_brewer(palette="Dark2") +
  guides(colour = FALSE) +
  xlab("Methods") +
  ylab("Coverage (Monte-Carlo SE)") + 
  ggtitle("Coverage with haz_shape = similar, eta1 = strong")
```


### 5. Nested loop plots - single

Main reference is [this paper](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-129) : nested loop plots (NLPs) can be really handy when we have a lot of scenarios to summarise.

I have made a variant of it which (I think) works rather well for this study.

The following 16 scenarios will be shown:

```{r nlp_tab}
scenarios %>% filter(
    miss_mech == "MAR",
    X_level == "continous",
    beta1 != "0") %>% 
  knitr::kable()
```

Here is the plot:

```{r nested_loop_X1}

# Attempt at nested loop plot for X1
dat_nlp <- all_estims %>% 
  filter(m %in% c(0, 50),
         var == "X.1",
         beta1 != "0",
         miss_mech != "MCAR")

# We only plot est, and the colours are the methods
# What is varied?
# - Prop miss (two levels)
# - Beta1 (0.5 and 1, two levels)
# - miss mech (4 levels, maybe use facets?)
# - eta1 (2 real levels, only relevant for three of the mechs)
# - haz shape two levels
# - Facet by X_level after

# Need library scales

# Order of nesting
#Propmiss then haz shape, then beta1, then eta1
dat_nlp_plot <- dat_nlp %>% 
  
 # You first order by facets, then nesting order 
 arrange(beta1, X_level, prop_miss, haz_shape, eta1) %>% 
  mutate(
    inter = interaction(
      analy, miss_mech, X_level, beta1, 
      prop_miss, haz_shape, eta1, 
      lex.order = T # important
    ),
    scenario = interaction(
      beta1, prop_miss, haz_shape, eta1
    )
  ) %>% 

  # Rescale vars 
  mutate(
    beta1 = as.numeric(as.character(beta1)),
    prop_miss = rescale(as.numeric(prop_miss), to = c(0.25, 0.3)),
    haz_shape = rescale(as.numeric(haz_shape), to = c(0.15, 0.20)),
    eta1 = rescale(as.numeric(eta1), to = c(0.05, 0.1))
  ) 


palo <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))


dat_nlp_plot %>% 
  filter(
    miss_mech == "MAR",
    X_level == "continuous"
  ) %>% 
  #group_by(miss_mech, X_level) %>% 
  mutate(
    scen = seq_along(inter) * 2 - 2
  ) %>%
  ggplot(aes(scen, est, shape = analy,
             col = scenario)) +
  geom_point(size = 2, alpha = 0.8) +
  ylim(c(0, 1.2)) + 
  geom_step(aes(x = scen, y = beta1, group = 1), col = "black",
            linetype = "dashed", alpha = 0.75) + 
  geom_step(aes(x = scen, y = prop_miss, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = haz_shape, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = eta1, group = 1), 
            col = "black") +
  annotate("text", label = "Missing: 10%, 50%", 
           x = 3, y = 0.33,
           size = 3, hjust = 0) +
  annotate("text", label = "Haz shape: similar, diff", 
           x = 3, y = 0.23,
           size = 3, hjust = 0) +
  annotate("text", label = "Eta1: weak, strong", 
           x = 3, y = 0.13,
           size = 3, hjust = 0) +
  coord_cartesian(expand = 0) + 
  guides(colour = F) +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_color_manual(values = palo(
    length(levels(dat_nlp_plot$scenario))
  )) + 
  ylab("Estimate (X.1)") +
  ggtitle("Nested loop plot for  MAR and contin X")  
#what am i looking at

# Make function! and look at below!
#https://journals.sagepub.com/doi/full/10.1177/0962280218780856
#https://github.com/matherealize/loopR

```

Scenarios are ordered along the x axis based on some manipulated variables (eta1, haz shape and prop_missing). From left to right you are looking at clusters of 6 points, corresponding to the six methods used. The scenario is grouped using point colour, and shape is use to tell the methods apart.

In the plot below (for example) we can identify a scenario as follows:

```{r}
dat_nlp_plot %>% 
  filter(
    miss_mech == "MAR",
    X_level == "continuous"
  ) %>% 
  #group_by(miss_mech, X_level) %>% 
  mutate(
    scen = seq_along(inter) * 2 - 2
  ) %>%
  ggplot(aes(scen, est, shape = analy,
             col = scenario)) +
    geom_rect(aes(xmin = 72, xmax = 84, ymin = 0, ymax = 1.2),
            alpha = 0.3, fill = "grey70", col = NA) +
  annotate("text", label = "Scenario 52 \n (50%, diff, strong)",
           x = 78, y = 0.8,
           size = 3, hjust = 0.5, angle = 90) +
  geom_point(size = 2, alpha = 0.8) +
  ylim(c(0, 1.2)) + 
  geom_step(aes(x = scen, y = beta1, group = 1), col = "black",
            linetype = "dashed", alpha = 0.75) + 
  geom_step(aes(x = scen, y = prop_miss, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = haz_shape, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = eta1, group = 1), 
            col = "black") +
  annotate("text", label = "Missing: 10%, 50%", 
           x = 3, y = 0.33,
           size = 3, hjust = 0) +
  annotate("text", label = "Haz shape: similar, diff", 
           x = 3, y = 0.23,
           size = 3, hjust = 0) +
  annotate("text", label = "Eta1: weak, strong", 
           x = 3, y = 0.13,
           size = 3, hjust = 0) +
  coord_cartesian(expand = 0) + 
  guides(colour = F) +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_color_manual(values = palo(
    length(levels(dat_nlp_plot$scenario))
  )) +  
  ylab("Estimate (X.1)") +
  ggtitle("Nested loop plot for  MAR and contin X") 
```

Just by looking across the plot, you can already make some tentative statements (which of course will need further investigation). In this case, for the estimated $\hat{\beta_1}$ with missingness at random and continous $X$:

* Bias is present almost exclusively for the MICE-based methods ($CH_{1}$, $CH_{12}$ and $CH_{12_\text{int}}$). CCA, and smcfcs mostly unbiased.

* Bias is greater when $\beta_1 = 1$ compared to $\beta_1 = 0.5$ (look at left half of the plot vs right half).

* Bias is greater when proportion missingness is higher (compare the halves *within* each half of the plot).

* Bias seems greater when the baseline hazard shapes are very different.

* A stronger MAR seems to improve performance of the MICE-based methods.


### 5. Nested loop plots - facetted

If we go one step further and use facets, we can display a whopping **96 scenarios** in one plot!! Of course this plot should only be use as a general impression, but is still super useful.

Given how cramped it looks on a vertical html, this would be plotted full page on an A4 in landscape.

```{r nlp_one}
dat_nlp_plot %>% 
  group_by(miss_mech, X_level) %>% 
  mutate(
    scen = seq_along(inter) * 2
  ) %>%
  ggplot(aes(scen, est, shape = analy,
             col = scenario)) +
  geom_point(size = 2, alpha = 0.8) +
  ylim(c(0, 1.2)) + 
  geom_step(aes(x = scen, y = beta1, group = 1), col = "black",
            linetype = "dashed", alpha = 0.75) + 
  geom_step(aes(x = scen, y = prop_miss, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = haz_shape, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = eta1, group = 1), 
            col = "black") +
  annotate("text", label = "Missing: 10%, 50%", 
           x = 3, y = 0.33,
           size = 3, hjust = 0) +
  annotate("text", label = "Haz shape: similar, diff", 
           x = 3, y = 0.23,
           size = 3, hjust = 0) +
  annotate("text", label = "Eta1: weak, strong", 
           x = 3, y = 0.13,
           size = 3, hjust = 0) +
  facet_grid(X_level ~ miss_mech) + # X_level * eta
  coord_cartesian(expand = 0) + 
  guides(colour = F) +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_color_manual(values = palo(
    length(levels(dat_nlp_plot$scenario))
  )) + 
  ylab("Estimate") 
```


## Extra: the 'traditional' NLP

Really not suitable for this study in my opinion. Plus rather lelijk

```{r}
# Traditional nested loop
dat_nlp_plot <- dat_nlp %>% 
  
 # You first order by facets, then nesting order 
 arrange(beta1, X_level, prop_miss, haz_shape, eta1) %>% 
  mutate(
    inter = interaction(
      miss_mech, X_level, beta1, 
      prop_miss, haz_shape, eta1, 
      lex.order = T # important
    ),
    scenario = interaction(
      beta1, prop_miss, haz_shape, eta1, lex.order = T
    )
  ) %>% 

  # Rescale vars 
  mutate(
    beta1 = as.numeric(as.character(beta1)),
    prop_miss = rescale(as.numeric(prop_miss), to = c(0.25, 0.3)),
    haz_shape = rescale(as.numeric(haz_shape), to = c(0.15, 0.20)),
    eta1 = rescale(as.numeric(eta1), to = c(0.05, 0.1))
  ) 

library(RColorBrewer)

palo <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))

dat_nlp_plot %>% 
  group_by(miss_mech, X_level) %>% 
  mutate(
    scen = as.numeric(scenario),
    scen = scen - scen[1] + 1 #seq_along(inter) * 2
  ) %>%
  ggplot(aes(scen, est, col = analy, group = analy)) +
  geom_step(size = 1.5, alpha = .9) +
  ylim(c(0, 1.2)) + 
  geom_step(aes(x = scen, y = beta1, group = 1), col = "black",
            linetype = "dashed", alpha = 0.9) + 
  geom_step(aes(x = scen, y = prop_miss, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = haz_shape, group = 1), 
            col = "black") +
  geom_step(aes(x = scen, y = eta1, group = 1), 
            col = "black") +
  annotate("text", label = "Missing: 10%, 50%", 
           x = 3, y = 0.33,
           size = 3, hjust = 0) +
  annotate("text", label = "Haz shape: similar, diff", 
           x = 3, y = 0.23,
           size = 3, hjust = 0) +
  annotate("text", label = "Eta1: weak, strong", 
           x = 3, y = 0.13,
           size = 3, hjust = 0) +
  facet_grid(X_level ~ miss_mech) + # X_level * eta
  coord_cartesian(expand = 0)  +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_color_manual(values = palo(
    length(levels(dat_nlp_plot$analy))
  )) + 
  ylab("Estimate")
```
