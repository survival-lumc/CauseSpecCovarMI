
@article{adesMyelodysplasticSyndromes2014,
  title = {Myelodysplastic Syndromes},
  author = {Ad{\`e}s, Lionel and Itzykson, Raphael and Fenaux, Pierre},
  year = {2014},
  month = jun,
  volume = {383},
  pages = {2239--2252},
  issn = {0140-6736},
  abstract = {Myelodysplastic syndromes are clonal marrow stem-cell disorders, characterised by ineffective haemopoiesis leading to blood cytopenias, and by progression to acute myeloid leukaemia in a third of patients. 15\% of cases occur after chemotherapy or radiotherapy for a previous cancer; the syndromes are most common in elderly people. The pathophysiology involves cytogenetic changes with or without gene mutations and widespread gene hypermethylation at advanced stages. Clinical manifestations result from cytopenias (anaemia, infection, and bleeding). Diagnosis is based on examination of blood and bone marrow showing blood cytopenias and hypercellular marrow with dysplasia, with or without excess of blasts. Prognosis depends largely on the marrow blast percentage, number and extent of cytopenias, and cytogenetic abnormalities. Treatment of patients with lower-risk myelodysplastic syndromes, especially for anaemia, includes growth factors, lenalidomide, and transfusions. Treatment of higher-risk patients is with hypomethylating agents and, whenever possible, allogeneic stem-cell transplantation.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\X38UTPUY\\Ad√®s et al. - 2014 - Myelodysplastic syndromes.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ZGHMB4UN\\S0140673613619017.html},
  journal = {The Lancet},
  language = {en},
  number = {9936}
}

@article{andersenCompetingRisksMultistate2016,
  title = {Competing Risks as a Multi-State Model},
  shorttitle = {Competing Risks as a Multi-State Model},
  author = {Andersen, Per Kragh and Abildstrom, Steen Z. and Rosth{\o}j, Susanne},
  year = {2016},
  month = jul,
  publisher = {{Sage PublicationsSage CA: Thousand Oaks, CA}},
  abstract = {This paper deals with the competing risks model as a special case of a multi-state model. The properties of the model are reviewed and contrasted to the so-call...},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\WHQ9RI66\\0962280202sm281ra.html},
  journal = {Statistical Methods in Medical Research},
  language = {en}
}

@article{bartlettMissingCovariatesCompeting2016,
  title = {Missing Covariates in Competing Risks Analysis},
  author = {Bartlett, Jonathan W. and Taylor, Jeremy M. G.},
  year = {2016},
  month = oct,
  volume = {17},
  pages = {751--763},
  publisher = {{Oxford Academic}},
  issn = {1465-4644},
  abstract = {Abstract.  Studies often follow individuals until they fail from one of a number of competing failure types. One approach to analyzing such competing risks data},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DUN3VNLD\\Bartlett and Taylor - 2016 - Missing covariates in competing risks analysis.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\P9DP2TVT\\2800456.html},
  journal = {Biostatistics},
  keywords = {missing-data},
  language = {en},
  number = {4}
}

@article{bartlettMultipleImputationCovariates2015,
  title = {Multiple Imputation of Covariates by Fully Conditional Specification: {{Accommodating}} the Substantive Model},
  shorttitle = {Multiple Imputation of Covariates by Fully Conditional Specification},
  author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R},
  year = {2015},
  month = aug,
  volume = {24},
  pages = {462--487},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  abstract = {Missing covariate data commonly occur in epidemiological and clinical research, and are often dealt with using multiple imputation. Imputation of partially observed covariates is complicated if the substantive model is non-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g. squared) or interaction terms, and standard software implementations of multiple imputation may impute covariates from models that are incompatible with such substantive models. We show how imputation by fully conditional specification, a popular approach for performing multiple imputation, can be modified so that covariates are imputed from models which are compatible with the substantive model. We investigate through simulation the performance of this proposal, and compare it with existing approaches. Simulation results suggest our proposal gives consistent estimates for a range of common substantive models, including models which contain non-linear covariate effects or interactions, provided data are missing at random and the assumed imputation models are correctly specified and mutually compatible. Stata software implementing the approach is freely available.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\QH7629W2\\Bartlett et al. - 2015 - Multiple imputation of covariates by fully conditi.pdf},
  journal = {Statistical Methods in Medical Research},
  language = {en},
  number = {4}
}

@manual{bartlettSmcfcsMultipleImputation2020,
  title = {Smcfcs: {{Multiple}} Imputation of Covariates by Substantive Model Compatible Fully Conditional Specification},
  author = {Bartlett, Jonathan and Keogh, Ruth},
  year = {2020},
  type = {Manual}
}

@book{beyersmannCompetingRisksMultistate2011,
  title = {Competing {{Risks}} and {{Multistate Models}} with {{R}}},
  author = {Beyersmann, Jan and Allignol, Arthur and Schumacher, Martin},
  year = {2011},
  month = nov,
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book covers competing risks and multistate models, sometimes summarized as event history analysis. These models generalize the analysis of time to a single event (survival analysis) to analysing the timing of distinct terminal events (competing risks) and possible intermediate events (multistate models. Both R and multistate methods are promoted with a focus on nonparametric methods.},
  googlebooks = {xQRic47kQZAC},
  isbn = {978-1-4614-2035-4},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  language = {en}
}

@article{beyersmannSimulatingCompetingRisks2009,
  title = {Simulating Competing Risks Data in Survival Analysis},
  author = {Beyersmann, Jan and Latouche, Aur{\'e}lien and Buchholz, Anika and Schumacher, Martin},
  year = {2009},
  volume = {28},
  pages = {956--971},
  issn = {1097-0258},
  abstract = {Competing risks analysis considers time-to-first-event (`survival time') and the event type (`cause'), possibly subject to right-censoring. The cause-, i.e. event-specific hazards, completely determine the competing risk process, but simulation studies often fall back on the much criticized latent failure time model. Cause-specific hazard-driven simulation appears to be the exception; if done, usually only constant hazards are considered, which will be unrealistic in many medical situations. We explain simulating competing risks data based on possibly time-dependent cause-specific hazards. The simulation design is as easy as any other, relies on identifiable quantities only and adds to our understanding of the competing risks process. In addition, it immediately generalizes to more complex multistate models. We apply the proposed simulation design to computing the least false parameter of a misspecified proportional subdistribution hazard model, which is a research question of independent interest in competing risks. The simulation specifications have been motivated by data on infectious complications in stem-cell transplanted patients, where results from cause-specific hazards analyses were difficult to interpret in terms of cumulative event probabilities. The simulation illustrates that results from a misspecified proportional subdistribution hazard analysis can be interpreted as a time-averaged effect on the cumulative event probability scale. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3516},
  copyright = {Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\EMMZKBFS\\Beyersmann et al. - 2009 - Simulating competing risks data in survival analys.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YPXF9TVM\\sim.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {6}
}

@article{boosAssessingVariabilityComplex2015,
  title = {Assessing {{Variability}} of {{Complex Descriptive Statistics}} in {{Monte Carlo Studies Using Resampling Methods}}},
  author = {Boos, Dennis D. and Osborne, Jason A.},
  year = {2015},
  volume = {83},
  pages = {228--238},
  issn = {1751-5823},
  abstract = {SummaryGood statistical practice dictates that summaries in Monte Carlo studies should always be accompanied by standard errors. Those standard errors are easy to provide for summaries that are sample means over the replications of the Monte Carlo output: for example, bias estimates, power estimates for tests and mean squared error estimates. But often more complex summaries are of interest: medians (often displayed in boxplots), sample variances, ratios of sample variances and non-normality measures such as skewness and kurtosis. In principle, standard errors for most of these latter summaries may be derived from the Delta Method, but that extra step is often a barrier for standard errors to be provided. Here, we highlight the simplicity of using the jackknife and bootstrap to compute these standard errors, even when the summaries are somewhat complicated. \textcopyright{} 2014 The Authors. International Statistical Review \textcopyright{} 2014 International Statistical Institute},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12087},
  copyright = {\textcopyright 2014\,The Authors. International Statistical Review \textcopyright{} 2014\,International Statistical Institute},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\NEKHA6VM\\Boos and Osborne - 2015 - Assessing Variability of Complex Descriptive Stati.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\WLWPHUJI\\insr.html},
  journal = {International Statistical Review},
  keywords = {Bootstrap,coefficient of variation,delta method,influence curve,jackknife,standard errors,variability of ratios},
  language = {en},
  number = {2}
}

@article{buurenMiceMultivariateImputation2011,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {van Buuren, Stef and {Groothuis-Oudshoorn}, Karin},
  year = {2011},
  month = dec,
  volume = {45},
  pages = {1--67},
  issn = {1548-7660},
  copyright = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\3XQ76XXS\\Buuren and Groothuis-Oudshoorn - 2011 - mice Multivariate Imputation by Chained Equations.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\2XQ6MU9F\\v045i03.html},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {1}
}

@article{carrollHowAreMissing2020,
  title = {How Are Missing Data in Covariates Handled in Observational Time-to-Event Studies in Oncology? {{A}} Systematic Review},
  shorttitle = {How Are Missing Data in Covariates Handled in Observational Time-to-Event Studies in Oncology?},
  author = {Carroll, Orlagh U. and Morris, Tim P. and Keogh, Ruth H.},
  year = {2020},
  month = may,
  volume = {20},
  pages = {134},
  issn = {1471-2288},
  abstract = {Missing data in covariates can result in biased estimates and loss of power to detect associations. It can also lead to other challenges in time-to-event analyses including the handling of time-varying effects of covariates, selection of covariates and their flexible modelling. This review aims to describe how researchers approach time-to-event analyses with missing data.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CXIWDHBA\\Carroll et al. - 2020 - How are missing data in covariates handled in obse.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\UVZ3PZ2Q\\s12874-020-01018-7.html},
  journal = {BMC Medical Research Methodology},
  number = {1}
}

@article{coxPartialLikelihood1975,
  title = {Partial Likelihood},
  author = {Cox, D. R.},
  year = {1975},
  month = aug,
  volume = {62},
  pages = {269--276},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  abstract = {Abstract.  A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and infer},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\THFHEYE2\\Cox - 1975 - Partial likelihood.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\2BK6NLZS\\337051.html},
  journal = {Biometrika},
  language = {en},
  number = {2}
}

@article{demirtasComputingPointbiserialCorrelation2016,
  title = {Computing the {{Point}}-Biserial {{Correlation}} under {{Any Underlying Continuous Distribution}}},
  author = {Demirtas, Hakan and Hedeker, Donald},
  year = {2016},
  month = sep,
  volume = {45},
  pages = {2744--2751},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  abstract = {The connection between the point-biserial and biserial correlations is well-established when the underlying distribution is bivariate normal. For many other bivariate distributions, the formula that links these two quantities is not straightforward to derive or does not have a closed form. We propose a simple technique that enables researchers to compute one of these correlations when the other is specified. For this, we take advantage of the constancy of their ratio, which can be easily approximated for any distribution. We illustrate the proposed method using several examples and discuss its extension to the ordinal case. We believe that this approach is potentially useful in stochastic simulation..},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2014.920883},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\FIFRPS72\\Demirtas and Hedeker - 2016 - Computing the Point-biserial Correlation under Any.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\FGY2R8UL\\03610918.2014.html},
  journal = {Communications in Statistics - Simulation and Computation},
  keywords = {Dichotomization,Primary 62F40,Secondary 62P10,Simulation,Sorting},
  number = {8}
}

@article{dewreedeMstatePackageAnalysis2011,
  title = {Mstate: {{An R Package}} for the {{Analysis}} of {{Competing Risks}} and {{Multi}}-{{State Models}}},
  shorttitle = {Mstate},
  author = {{de Wreede}, Liesbeth C. and Fiocco, Marta and Putter, Hein},
  year = {2011},
  month = jan,
  volume = {38},
  pages = {1--30},
  issn = {1548-7660},
  copyright = {Copyright (c) 2010 Liesbeth C. de Wreede, Marta Fiocco, Hein Putter},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\WDPTX5AZ\\Wreede et al. - 2011 - mstate An R Package for the Analysis of Competing.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\LW49KM7C\\v038i07.html},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {1}
}

@article{dewreedeMstatePackageEstimation2010a,
  title = {The Mstate Package for Estimation and Prediction in Non- and Semi-Parametric Multi-State and Competing Risks Models},
  author = {{de Wreede}, Liesbeth C. and Fiocco, Marta and Putter, Hein},
  year = {2010},
  month = sep,
  volume = {99},
  pages = {261--274},
  issn = {0169-2607},
  abstract = {In recent years, multi-state models have been studied widely in survival analysis. Despite their clear advantages, their use in biomedical and other applications has been rather limited so far. An important reason for this is the lack of flexible and user-friendly software for multi-state models. This paper introduces a package in R, called `mstate', for each of the steps of the analysis of multi-state models. It can be applied to non- and semi-parametric models. The package contains functions to facilitate data preparation and flexible estimation of different types of covariate effects in the context of Cox regression models, functions to estimate patient-specific transition intensities, dynamic prediction probabilities and their associated standard errors (both Greenwood and Aalen-type). Competing risks models can also be analyzed by means of mstate, as they are a special type of multi-state models. The package is available from the R homepage http://cran.r-project.org. We give a self-contained account of the underlying mathematical theory, including a new asymptotic result for the cumulative hazard function and new recursive formulas for the calculation of the estimated standard errors of the estimated transition probabilities, and we illustrate the use of the key functions of the mstate package by the analysis of a reversible multi-state model describing survival of liver cirrhosis patients.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\BKJDL83D\\de Wreede et al. - 2010 - The mstate package for estimation and prediction i.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\AEEF3S8X\\S0169260710000027.html},
  journal = {Computer Methods and Programs in Biomedicine},
  keywords = {Competing risks models,Cox models,Markov models,Multi-state models,Software,Survival analysis},
  language = {en},
  number = {3}
}

@article{erlerDealingMissingCovariates2016,
  title = {Dealing with Missing Covariates in Epidemiologic Studies: A Comparison between Multiple Imputation and a Full {{Bayesian}} Approach},
  shorttitle = {Dealing with Missing Covariates in Epidemiologic Studies},
  author = {Erler, Nicole S. and Rizopoulos, Dimitris and van Rosmalen, Joost and Jaddoe, Vincent W. V. and Franco, Oscar H. and Lesaffre, Emmanuel M. E. H.},
  year = {2016},
  volume = {35},
  pages = {2955--2974},
  issn = {1097-0258},
  abstract = {Incomplete data are generally a challenge to the analysis of most large studies. The current gold standard to account for missing data is multiple imputation, and more specifically multiple imputation with chained equations (MICE). Numerous studies have been conducted to illustrate the performance of MICE for missing covariate data. The results show that the method works well in various situations. However, less is known about its performance in more complex models, specifically when the outcome is multivariate as in longitudinal studies. In current practice, the multivariate nature of the longitudinal outcome is often neglected in the imputation procedure, or only the baseline outcome is used to impute missing covariates. In this work, we evaluate the performance of MICE using different strategies to include a longitudinal outcome into the imputation models and compare it with a fully Bayesian approach that jointly imputes missing values and estimates the parameters of the longitudinal model. Results from simulation and a real data example show that MICE requires the analyst to correctly specify which components of the longitudinal process need to be included in the imputation models in order to obtain unbiased results. The full Bayesian approach, on the other hand, does not require the analyst to explicitly specify how the longitudinal outcome enters the imputation models. It performed well under different scenarios. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6944},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\T8ALCJ9Y\\Erler et al. - 2016 - Dealing with missing covariates in epidemiologic s.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\V4NP32LD\\sim.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {17}
}

@article{erlerJointAIJointAnalysis2020,
  title = {{{JointAI}}: {{Joint Analysis}} and {{Imputation}} of {{Incomplete Data}} in {{R}}},
  shorttitle = {{{JointAI}}},
  author = {Erler, Nicole S. and Rizopoulos, Dimitris and Lesaffre, Emmanuel M. E. H.},
  year = {2020},
  month = sep,
  abstract = {Missing data occur in many types of studies and typically complicate the analysis. Multiple imputation, either using joint modelling or the more flexible fully conditional specification approach, are popular and work well in standard settings. In settings involving non-linear associations or interactions, however, incompatibility of the imputation model with the analysis model is an issue often resulting in bias. Similarly, complex outcomes such as longitudinal or survival outcomes cannot be adequately handled by standard implementations. In this paper, we introduce the R package JointAI, which utilizes the Bayesian framework to perform simultaneous analysis and imputation in regression models with incomplete covariates. Using a fully Bayesian joint modelling approach it overcomes the issue of uncongeniality while retaining the attractive flexibility of fully conditional specification multiple imputation by specifying the joint distribution of analysis and imputation models as a sequence of univariate models that can be adapted to the type of variable. JointAI provides functions for Bayesian inference with generalized linear and generalized linear mixed models and extensions thereof as well as survival models and joint models for longitudinal and survival data, that take arguments analogous to corresponding well known functions for the analysis of complete data from base R and other packages. Usage and features of JointAI are described and illustrated using various examples and the theoretical background is outlined.},
  archivePrefix = {arXiv},
  eprint = {1907.10867},
  eprinttype = {arxiv},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\TG7KVL76\\Erler et al. - 2020 - JointAI Joint Analysis and Imputation of Incomple.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\XN5AJ33S\\1907.html},
  journal = {arXiv:1907.10867 [stat]},
  keywords = {Statistics - Computation,Statistics - Methodology},
  primaryClass = {stat}
}

@article{falcaroEstimatingExcessHazard2015,
  title = {Estimating {{Excess Hazard Ratios}} and {{Net Survival When Covariate Data Are Missing}}: {{Strategies}} for {{Multiple Imputation}}},
  shorttitle = {Estimating {{Excess Hazard Ratios}} and {{Net Survival When Covariate Data Are Missing}}},
  author = {Falcaro, Milena and Nur, Ula and Rachet, Bernard and Carpenter, James R.},
  year = {2015},
  month = may,
  volume = {26},
  pages = {421--428},
  issn = {1044-3983},
  abstract = {Background:~         Net survival is the survival probability we would observe if the disease under study were the only cause of death. When estimated from routinely collected population-based cancer registry data, this indicator is a key metric for cancer control. Unfortunately, such data typically contain a non-negligible proportion of missing values on important prognostic factors (eg, tumor stage).         Methods:~         We carried out an empirical study to compare the performance of complete records analysis and several multiple imputation strategies when net survival is estimated via a flexible parametric proportional hazards model that includes stage, a partially observed categorical covariate. Starting from fully observed cancer registry data, we induced missingness on stage under three scenarios. For each of these scenarios, we simulated 100 incomplete datasets and evaluated the performance of the different strategies.         Results:~         Ordinal logistic models are not suitable for the imputation of tumor stage. Complete records analysis may lead to grossly misleading estimates of net survival, even when the missing data mechanism is conditionally independent of survival time given the covariates and the bias on the excess hazard ratios estimates is negligible.         Conclusions:~         As key covariates are unlikely missing completely at random, studies estimating net survival should not use complete records. When the missingness can be inferred from available data, appropriate multiple imputation should be performed. In the context of flexible parametric proportional hazards models with a partially observed stage covariate, a multinomial logistic imputation model for stage should be used and should include the Nelson-Aalen cumulative hazard estimate and the event indicator.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9AZ76GRG\\Estimating_Excess_Hazard_Ratios_and_Net_Survival.19.html},
  journal = {Epidemiology},
  language = {en-US},
  number = {3}
}

@article{fineProportionalHazardsModel1999,
  title = {A {{Proportional Hazards Model}} for the {{Subdistribution}} of a {{Competing Risk}}},
  author = {Fine, Jason P. and Gray, Robert J.},
  year = {1999},
  month = jun,
  volume = {94},
  pages = {496--509},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10474144},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\7NX5JAYT\\Fine and Gray - 1999 - A Proportional Hazards Model for the Subdistributi.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\MMGICLU8\\01621459.1999.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Hazard of subdistribution,Martingale,Partial likelihood,Transformation model},
  number = {446}
}

@article{geraciMultipleImputationBounded2018,
  title = {Multiple {{Imputation}} for {{Bounded Variables}}},
  author = {Geraci, Marco and McLain, Alexander},
  year = {2018},
  month = dec,
  volume = {83},
  pages = {919--940},
  issn = {1860-0980},
  abstract = {Missing data are a common issue in statistical analyses. Multiple imputation is a technique that has been applied in countless research studies and has a strong theoretical basis. Most of the statistical literature on multiple imputation has focused on unbounded continuous variables, with mostly ad hoc remedies for variables with bounded support. These approaches can be unsatisfactory when applied to bounded variables as they can produce misleading inferences. In this paper, we propose a flexible quantile-based imputation model suitable for distributions defined over singly or doubly bounded intervals. Proper support of the imputed values is ensured by applying a family of transformations with singly or doubly bounded range. Simulation studies demonstrate that our method is able to deal with skewness, bimodality, and heteroscedasticity and has superior properties as compared to competing approaches, such as log-normal imputation and predictive mean matching. We demonstrate the application of the proposed imputation procedure by analysing data on mathematical development scores in children from the Millennium Cohort Study, UK. We also show a specific advantage of our methods using a small psychiatric dataset. Our methods are relevant in a number of fields, including education and psychology.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\IAMQVYQ4\\Geraci and McLain - 2018 - Multiple Imputation for Bounded Variables.pdf},
  journal = {Psychometrika},
  language = {en},
  number = {4}
}

@article{grambauerProportionalSubdistributionHazards2010,
  title = {Proportional Subdistribution Hazards Modeling Offers a Summary Analysis, Even If Misspecified},
  author = {Grambauer, Nadine and Schumacher, Martin and Beyersmann, Jan},
  year = {2010},
  volume = {29},
  pages = {875--884},
  issn = {1097-0258},
  abstract = {Competing risks model time-to-first-event and the event type. Our motivating data example is the ONKO-KISS study on the occurrence of infections in neutropenic patients after stem-cell transplantation with first-event-types `infection' and `end of neutropenia'. The standard approach to study the effects of covariates in competing risks is to assume each event-specific hazard (ESH) to follow a proportional hazards model. However, a summarizing probability interpretation of the different event-specific effects of one covariate can be challenging. This difficulty has led to the development of the proportional subdistribution hazards model of a competing event of interest. However, one model specification usually precludes the other. Assuming proportional ESHs, we find that the subdistribution log-hazard ratio may show a pronounced time-dependency, even changing sign. Still, the subdistribution analysis is useful by estimating the least false parameter (LFP), a time-averaged effect on the cumulative event probabilities. In examples, we find that the LFP offers a robust summary of the effects on the ESHs for different observation periods, ranging from heavy censoring to no censoring at all. In particular, if there is no effect on the competing ESH, the subdistribution log-hazard ratio is close to the event-specific log-hazard ratio of interest. We reanalyze an interpretationally challenging example from the ONKO-KISS study and conduct a simulation study, where we find that the LFP is reliably estimated by the subdistribution analysis even for moderate sample sizes. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3786},
  copyright = {Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\Z5TFJ9NI\\Grambauer et al. - 2010 - Proportional subdistribution hazards modeling offe.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YGYYB5L2\\sim.html},
  journal = {Statistics in Medicine},
  keywords = {cause-specific hazard,competing risks,Cox model,Fine and Gray model,model misspecification},
  language = {en},
  number = {7-8}
}

@article{keoghMultipleImputationCox2018,
  title = {Multiple Imputation in {{Cox}} Regression When There Are Time-Varying Effects of Covariates},
  author = {Keogh, Ruth H. and Morris, Tim P.},
  year = {2018},
  volume = {37},
  pages = {3661--3678},
  issn = {1097-0258},
  abstract = {In Cox regression, it is important to test the proportional hazards assumption and sometimes of interest in itself to study time-varying effects (TVEs) of covariates. TVEs can be investigated with log hazard ratios modelled as a function of time. Missing data on covariates are common and multiple imputation is a popular approach to handling this to avoid the potential bias and efficiency loss resulting from a ``complete-case'' analysis. Two multiple imputation methods have been proposed for when the substantive model is a Cox proportional hazards regression: an approximate method (Imputing missing covariate values for the Cox model in Statistics in Medicine (2009) by White and Royston) and a substantive-model-compatible method (Multiple imputation of covariates by fully conditional specification: accommodating the substantive model in Statistical Methods in Medical Research (2015) by Bartlett et al). At present, neither accommodates TVEs of covariates. We extend them to do so for a general form for the TVEs and give specific details for TVEs modelled using restricted cubic splines. Simulation studies assess the performance of the methods under several underlying shapes for TVEs. Our proposed methods give approximately unbiased TVE estimates for binary covariates with missing data, but for continuous covariates, the substantive-model-compatible method performs better. The methods also give approximately correct type I errors in the test for proportional hazards when there is no TVE and gain power to detect TVEs relative to complete-case analysis. Ignoring TVEs at the imputation stage results in biased TVE estimates, incorrect type I errors, and substantial loss of power in detecting TVEs. We also propose a multivariable TVE model selection algorithm. The methods are illustrated using data from the Rotterdam Breast Cancer Study. R code is provided.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7842},
  copyright = {\textcopyright{} 2018 The Authors. Statistics~in~Medicine Published by John Wiley \& Sons Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\MYQ253B8\\Keogh and Morris - 2018 - Multiple imputation in Cox regression when there a.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\YVFGGWB6\\sim.html},
  journal = {Statistics in Medicine},
  keywords = {Cox regression,missing data,multiple imputation,restricted cubic spline,time-varying effect},
  language = {en},
  number = {25}
}

@article{keoghMultipleImputationCox2018a,
  title = {Multiple Imputation in {{Cox}} Regression When There Are Time-Varying Effects of Covariates},
  author = {Keogh, Ruth H. and Morris, Tim P.},
  year = {2018},
  volume = {37},
  pages = {3661--3678},
  issn = {1097-0258},
  abstract = {In Cox regression, it is important to test the proportional hazards assumption and sometimes of interest in itself to study time-varying effects (TVEs) of covariates. TVEs can be investigated with log hazard ratios modelled as a function of time. Missing data on covariates are common and multiple imputation is a popular approach to handling this to avoid the potential bias and efficiency loss resulting from a ``complete-case'' analysis. Two multiple imputation methods have been proposed for when the substantive model is a Cox proportional hazards regression: an approximate method (Imputing missing covariate values for the Cox model in Statistics in Medicine (2009) by White and Royston) and a substantive-model-compatible method (Multiple imputation of covariates by fully conditional specification: accommodating the substantive model in Statistical Methods in Medical Research (2015) by Bartlett et al). At present, neither accommodates TVEs of covariates. We extend them to do so for a general form for the TVEs and give specific details for TVEs modelled using restricted cubic splines. Simulation studies assess the performance of the methods under several underlying shapes for TVEs. Our proposed methods give approximately unbiased TVE estimates for binary covariates with missing data, but for continuous covariates, the substantive-model-compatible method performs better. The methods also give approximately correct type I errors in the test for proportional hazards when there is no TVE and gain power to detect TVEs relative to complete-case analysis. Ignoring TVEs at the imputation stage results in biased TVE estimates, incorrect type I errors, and substantial loss of power in detecting TVEs. We also propose a multivariable TVE model selection algorithm. The methods are illustrated using data from the Rotterdam Breast Cancer Study. R code is provided.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7842},
  copyright = {\textcopyright{} 2018 The Authors. Statistics~in~Medicine Published by John Wiley \& Sons Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\Z9V3TJ6K\\Keogh and Morris - 2018 - Multiple imputation in Cox regression when there a.pdf},
  journal = {Statistics in Medicine},
  keywords = {Cox regression,missing data,multiple imputation,restricted cubic spline,time-varying effect},
  language = {en},
  number = {25}
}

@book{kleinSurvivalAnalysisTechniques2006,
  title = {Survival {{Analysis}}: {{Techniques}} for {{Censored}} and {{Truncated Data}}},
  shorttitle = {Survival {{Analysis}}},
  author = {Klein, John P. and Moeschberger, Melvin L.},
  year = {2006},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Applied statisticians in many fields frequently analyze time-to-event data. While the statistical tools presented in this book are applicable to data from medicine, biology, public health, epidemiology, engineering, economics and demography, the focus here is on applications of the techniques to biology and medicine. The analysis of survival experiments is complicated by issues of censoring and truncation. The use of counting process methodology has allowed for substantial advances in the statistical theory to account for censoring and truncation in survival experiments. This book makes these complex techniques accessible to applied researchers without the advanced mathematical background. The authors present the essentials of these techniques, as well as classical techniques not based on counting processes, and apply them to data. The second edition contains some new material as well as solutions to the odd-numbered revised exercises. New material consists of a discussion of summary statistics for competing risks probabilities in Chapter 2 and the estimation process for these probabilities in Chapter 4. A new section on tests of the equality of survival curves at a fixed point in time is added in Chapter 7. In Chapter 8 an expanded discussion is presented on how to code covariates and a new section on discretizing a continuous covariate is added. A new section on Lin and Ying's additive hazards regression model is presented in Chapter 10. We now proceed to a general discussion of the usefulness of this book incorporating the new material with that of the first edition.},
  googlebooks = {aO7xBwAAQBAJ},
  isbn = {978-0-387-21645-4},
  language = {en}
}

@article{lauMissingnessSettingCompeting2018,
  title = {Missingness in the {{Setting}} of {{Competing Risks}}: From {{Missing Values}} to {{Missing Potential Outcomes}}},
  shorttitle = {Missingness in the {{Setting}} of {{Competing Risks}}},
  author = {Lau, Bryan and Lesko, Catherine},
  year = {2018},
  month = jun,
  volume = {5},
  pages = {153--159},
  issn = {2196-2995},
  abstract = {The setting of competing risks in which there is an event that precludes the event of interest from occurring is prevalent in epidemiological research. Unless studying all-cause mortality, any study following up individuals is subject to having a competing risk should individuals die during time period that the study covers. While there are prior papers discussing the need for competing risk methods in epidemiologic research, we are not aware of any review that discusses issues of missing data in a competing risk setting.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\BL6W9EZK\\Lau and Lesko - 2018 - Missingness in the Setting of Competing Risks fro.pdf},
  journal = {Current Epidemiology Reports},
  language = {en},
  number = {2}
}

@article{leeMultipleImputationPresence2017,
  title = {Multiple Imputation in the Presence of Non-Normal Data},
  author = {Lee, Katherine J. and Carlin, John B.},
  year = {2017},
  volume = {36},
  pages = {606--617},
  issn = {1097-0258},
  abstract = {Multiple imputation (MI) is becoming increasingly popular for handling missing data. Standard approaches for MI assume normality for continuous variables (conditionally on the other variables in the imputation model). However, it is unclear how to impute non-normally distributed continuous variables. Using simulation and a case study, we compared various transformations applied prior to imputation, including a novel non-parametric transformation, to imputation on the raw scale and using predictive mean matching (PMM) when imputing non-normal data. We generated data from a range of non-normal distributions, and set 50\% to missing completely at random or missing at random. We then imputed missing values on the raw scale, following a zero-skewness log, Box\textendash Cox or non-parametric transformation and using PMM with both type 1 and 2 matching. We compared inferences regarding the marginal mean of the incomplete variable and the association with a fully observed outcome. We also compared results from these approaches in the analysis of depression and anxiety symptoms in parents of very preterm compared with term-born infants. The results provide novel empirical evidence that the decision regarding how to impute a non-normal variable should be based on the nature of the relationship between the variables of interest. If the relationship is linear in the untransformed scale, transformation can introduce bias irrespective of the transformation used. However, if the relationship is non-linear, it may be important to transform the variable to accurately capture this relationship. A useful alternative is to impute the variable using PMM with type 1 matching. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7173},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\JH8PJ3IA\\Lee and Carlin - 2017 - Multiple imputation in the presence of non-normal .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\TT7UAVT4\\sim.html},
  journal = {Statistics in Medicine},
  keywords = {missing data,multiple imputation,non-normal data,predictive mean matching,transformation},
  language = {en},
  number = {4}
}

@article{madley-dowdProportionMissingData2019,
  title = {The Proportion of Missing Data Should Not Be Used to Guide Decisions on Multiple Imputation},
  author = {{Madley-Dowd}, Paul and Hughes, Rachael and Tilling, Kate and Heron, Jon},
  year = {2019},
  month = jun,
  volume = {110},
  pages = {63--73},
  issn = {0895-4356},
  abstract = {Objectives Researchers are concerned whether multiple imputation (MI) or complete case analysis should be used when a large proportion of data are missing. We aimed to provide guidance for drawing conclusions from data with a large proportion of missingness. Study Design and Setting Via simulations, we investigated how the proportion of missing data, the fraction of missing information (FMI), and availability of auxiliary variables affected MI performance. Outcome data were missing completely at random or missing at random (MAR). Results Provided sufficient auxiliary information was available; MI was beneficial in terms of bias and never detrimental in terms of efficiency. Models with similar FMI values, but differing proportions of missing data, also had similar precision for effect estimates. In the absence of bias, the FMI was a better guide to the efficiency gains using MI than the proportion of missing data. Conclusion We provide evidence that for MAR data, valid MI reduces bias even when the proportion of missingness is large. We advise researchers to use FMI to guide choice of auxiliary variables for efficiency gain in imputation analyses, and that sensitivity analyses including different imputation models may be needed if the number of complete cases is small.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\VYYKXACR\\Madley-Dowd et al. - 2019 - The proportion of missing data should not be used .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\JWNHE6EY\\S0895435618308710.html},
  journal = {Journal of Clinical Epidemiology},
  keywords = {ALSPAC,Bias,Methods,Missing data,Multiple imputation,Simulation},
  language = {en}
}

@article{marshallCombiningEstimatesInterest2009,
  title = {Combining Estimates of Interest in Prognostic Modelling Studies after Multiple Imputation: Current Practice and Guidelines},
  shorttitle = {Combining Estimates of Interest in Prognostic Modelling Studies after Multiple Imputation},
  author = {Marshall, Andrea and Altman, Douglas G. and Holder, Roger L. and Royston, Patrick},
  year = {2009},
  month = jul,
  volume = {9},
  pages = {57},
  issn = {1471-2288},
  abstract = {Multiple imputation (MI) provides an effective approach to handle missing covariate data within prognostic modelling studies, as it can properly account for the missing data uncertainty. The multiply imputed datasets are each analysed using standard prognostic modelling techniques to obtain the estimates of interest. The estimates from each imputed dataset are then combined into one overall estimate and variance, incorporating both the within and between imputation variability. Rubin's rules for combining these multiply imputed estimates are based on asymptotic theory. The resulting combined estimates may be more accurate if the posterior distribution of the population parameter of interest is better approximated by the normal distribution. However, the normality assumption may not be appropriate for all the parameters of interest when analysing prognostic modelling studies, such as predicted survival probabilities and model performance measures.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\67UH2WW9\\Marshall et al. - 2009 - Combining estimates of interest in prognostic mode.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\ECIBZKHM\\1471-2288-9-57.html},
  journal = {BMC Medical Research Methodology},
  keywords = {Imputation Model,Impute Dataset,Likelihood Ratio Statistic,Multiple Imputation,Prognostic Modelling},
  number = {1}
}

@article{marshallComparisonImputationMethods2010,
  title = {Comparison of Imputation Methods for Handling Missing Covariate Data When Fitting a {{Cox}} Proportional Hazards Model: A Resampling Study},
  shorttitle = {Comparison of Imputation Methods for Handling Missing Covariate Data When Fitting a {{Cox}} Proportional Hazards Model},
  author = {Marshall, Andrea and Altman, Douglas G. and Holder, Roger L.},
  year = {2010},
  month = dec,
  volume = {10},
  pages = {112},
  issn = {1471-2288},
  abstract = {The appropriate handling of missing covariate data in prognostic modelling studies is yet to be conclusively determined. A resampling study was performed to investigate the effects of different missing data methods on the performance of a prognostic model.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\ZMBQ5KWS\\Marshall et al. - 2010 - Comparison of imputation methods for handling miss.pdf},
  journal = {BMC Medical Research Methodology},
  language = {en},
  number = {1}
}

@book{mccullagh1989generalized,
  title = {Generalized Linear Models, Second Edition},
  author = {McCullagh, P. and Nelder, J.A.},
  year = {1989},
  publisher = {{Chapman \& Hall}},
  added-at = {2012-11-18T14:31:40.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/23a882b729f3f7333923cd713869d3f5f/peter.ralph},
  interhash = {1c22cedb7c518df1a6b0999d3f04f629},
  intrahash = {3a882b729f3f7333923cd713869d3f5f},
  isbn = {978-0-412-31760-6},
  keywords = {GLM reference statistics},
  lccn = {99013896},
  series = {Chapman and {{Hall}}/{{CRC}} Monographs on Statistics and Applied Probability Series},
  timestamp = {2012-11-18T14:31:40.000+0100}
}

@article{mertensConstructionAssessmentPrediction2020,
  title = {Construction and Assessment of Prediction Rules for Binary Outcome in the Presence of Missing Predictor Data Using Multiple Imputation and Cross-Validation: {{Methodological}} Approach and Data-Based Evaluation},
  shorttitle = {Construction and Assessment of Prediction Rules for Binary Outcome in the Presence of Missing Predictor Data Using Multiple Imputation and Cross-Validation},
  author = {Mertens, Bart J. A. and Banzato, Erika and de Wreede, Liesbeth C.},
  year = {2020},
  volume = {62},
  pages = {724--741},
  issn = {1521-4036},
  abstract = {We investigate calibration and assessment of predictive rules when missing values are present in the predictors. Our paper has two key objectives. The first is to investigate how the calibration of the prediction rule can be combined with use of multiple imputation to account for missing predictor observations. The second objective is to propose such methods that can be implemented with current multiple imputation software, while allowing for unbiased predictive assessment through validation on new observations for which outcome is not yet available. We commence with a review of the methodological foundations of multiple imputation as a model estimation approach as opposed to a purely algorithmic description. We specifically contrast application of multiple imputation for parameter (effect) estimation with predictive calibration. Based on this review, two approaches are formulated, of which the second utilizes application of the classical Rubin's rules for parameter estimation, while the first approach averages probabilities from models fitted on single imputations to directly approximate the predictive density for future observations. We present implementations using current software that allow for validation and estimation of performance measures by cross-validation, as well as imputation of missing data in predictors on the future data where outcome is missing by definition. To simplify, we restrict discussion to binary outcome and logistic regression throughout. Method performance is verified through application on two real data sets. Accuracy (Brier score) and variance of predicted probabilities are investigated. Results show substantial reductions in variation of calibrated probabilities when using the first approach.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201800289},
  copyright = {\textcopyright{} 2020 The Authors. Biometrical Journal published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9SAL9JLV\\Mertens et al. - 2020 - Construction and assessment of prediction rules fo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\GNIJQZAW\\bimj.html},
  journal = {Biometrical Journal},
  language = {en},
  number = {3}
}

@article{milesObtainingPredictionsModels2016,
  title = {Obtaining {{Predictions}} from {{Models Fit}} to {{Multiply Imputed Data}}},
  author = {Miles, Andrew},
  year = {2016},
  month = feb,
  volume = {45},
  pages = {175--185},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  abstract = {Obtaining predictions from regression models fit to multiply imputed data can be challenging because treatments of multiple imputation seldom give clear guidance on how predictions can be calculated, and because available software often does not have built-in routines for performing the necessary calculations. This research note reviews how predictions can be obtained using Rubin's rules, that is, by being estimated separately in each imputed data set and then combined. It then demonstrates that predictions can also be calculated directly from the final analysis model. Both approaches yield identical results when predictions rely solely on linear transformations of the coefficients and calculate standard errors using the delta method and diverge only slightly when using nonlinear transformations. However, calculation from the final model is faster, easier to implement, and generates predictions with a clearer relationship to model coefficients. These principles are illustrated using data from the General Social Survey and with a simulation.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CSUP3ZDU\\Miles - 2016 - Obtaining Predictions from Models Fit to Multiply .pdf},
  journal = {Sociological Methods \& Research},
  keywords = {linear transformation,missing data,multiple imputation,nonlinear transformation,prediction},
  language = {en},
  number = {1}
}

@article{morisotProstateCancerNet2015,
  title = {Prostate Cancer: Net Survival and Cause-Specific Survival Rates after Multiple Imputation},
  shorttitle = {Prostate Cancer},
  author = {Morisot, Adeline and Bessaoud, Fa{\"i}za and Landais, Paul and R{\'e}billard, Xavier and Tr{\'e}tarre, Brigitte and Daur{\`e}s, Jean-Pierre},
  year = {2015},
  month = jul,
  volume = {15},
  pages = {54},
  issn = {1471-2288},
  abstract = {Estimations of survival rates are diverse and the choice of the appropriate method depends on the context. Given the increasing interest in multiple imputation methods, we explored the interest of a multiple imputation approach in the estimation of cause-specific survival, when a subset of causes of death was observed.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\VWVFAD2N\\Morisot et al. - 2015 - Prostate cancer net survival and cause-specific s.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\4VJDSRCD\\s12874-015-0048-4.html},
  journal = {BMC Medical Research Methodology},
  keywords = {Cause-specific survival,ERSPC,Multiple imputation,Net survival},
  number = {1}
}

@article{morrisUsingSimulationStudies2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  volume = {38},
  pages = {2074--2102},
  issn = {1097-0258},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some ``truth'' (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (``ADEMP''); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
  copyright = {\textcopyright{} 2019 The Authors. Statistics~in~Medicine Published by John Wiley \& Sons Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\TXYCHCGB\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\SB24XQN6\\sim.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {11}
}

@article{murrayMultipleImputationReview2018,
  title = {Multiple {{Imputation}}: {{A Review}} of {{Practical}} and {{Theoretical Findings}}},
  shorttitle = {Multiple {{Imputation}}},
  author = {Murray, Jared S.},
  year = {2018},
  month = may,
  volume = {33},
  pages = {142--159},
  issn = {0883-4237},
  abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in flexible joint modeling and sequential regression/chained equations/fully conditional specification approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\RSFPBU8S\\Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf},
  journal = {Statistical Science},
  language = {en},
  number = {2}
}

@article{nguyenModelCheckingMultiple2017,
  title = {Model Checking in Multiple Imputation: An Overview and Case Study},
  shorttitle = {Model Checking in Multiple Imputation},
  author = {Nguyen, Cattram D. and Carlin, John B. and Lee, Katherine J.},
  year = {2017},
  month = aug,
  volume = {14},
  pages = {8},
  issn = {1742-7622},
  abstract = {Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\UEUW2WZH\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf},
  journal = {Emerging Themes in Epidemiology},
  language = {en},
  number = {1}
}

@article{prenticeAnalysisFailureTimes1978,
  title = {The {{Analysis}} of {{Failure Times}} in the {{Presence}} of {{Competing Risks}}},
  author = {Prentice, R. L. and Kalbfleisch, J. D. and Peterson, A. V. and Flournoy, N. and Farewell, V. T. and Breslow, N. E.},
  year = {1978},
  volume = {34},
  pages = {541--554},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  abstract = {Distinct problems in the analysis of failure times with competing causes of failure include the estimation of treatment or exposure effects on specific failure types, the study of interrelations among failure types, and the estimation of failure rates for some causes given the removal of certain other failure types. The usual formulation of these problems is in terms of conceptual or latent failure times for each failure type. This approach is criticized on the basis of unwarranted assumptions, lack of physical interpretation and identifiability problems. An alternative approach utilizing cause-specific hazard functions for observable quantities, including time-dependent covariates, is proposed. Cause-specific hazard functions are shown to be the basic estimable quantities in the competing risks framework. A method, involving the estimation of parameters that relate time-dependent risk indicators for some causes to cause-specific hazard functions for other causes, is proposed for the study of interrelations among failure types. Further, it is argued that the problem of estimation of failure rates under the removal of certain causes is not well posed until a mechanism for cause removal is specified. Following such a specification, one will sometimes be in a position to make sensible extrapolations from available data to situations involving cause removal. A clinical program in bone marrow transplantation for leukemia provides a setting for discussion and illustration of each of these ideas. Failure due to censoring in a survivorship study leads to further discussion.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\Y3RS45TG\\Prentice et al. - 1978 - The Analysis of Failure Times in the Presence of C.pdf},
  journal = {Biometrics},
  number = {4}
}

@article{putterRelationCausespecificHazard2020a,
  title = {On the Relation between the Cause-Specific Hazard and the Subdistribution Rate for Competing Risks Data: {{The Fine}}\textendash{{Gray}} Model Revisited},
  shorttitle = {On the Relation between the Cause-Specific Hazard and the Subdistribution Rate for Competing Risks Data},
  author = {Putter, Hein and Schumacher, Martin and van Houwelingen, Hans C.},
  year = {2020},
  volume = {62},
  pages = {790--807},
  issn = {1521-4036},
  abstract = {The Fine\textendash Gray proportional subdistribution hazards model has been puzzling many people since its introduction. The main reason for the uneasy feeling is that the approach considers individuals still at risk for an event of cause 1 after they fell victim to the competing risk of cause 2. The subdistribution hazard and the extended risk sets, where subjects who failed of the competing risk remain in the risk set, are generally perceived as unnatural . One could say it is somewhat of a riddle why the Fine\textendash Gray approach yields valid inference. To take away these uneasy feelings, we explore the link between the Fine\textendash Gray and cause-specific approaches in more detail. We introduce the reduction factor as representing the proportion of subjects in the Fine\textendash Gray risk set that has not yet experienced a competing event. In the presence of covariates, the dependence of the reduction factor on a covariate gives information on how the effect of the covariate on the cause-specific hazard and the subdistribution hazard relate. We discuss estimation and modeling of the reduction factor, and show how they can be used in various ways to estimate cumulative incidences, given the covariates. Methods are illustrated on data of the European Society for Blood and Marrow Transplantation.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201800274},
  copyright = {\textcopyright{} 2020 The Authors. Biometrical Journal published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\3X95KGPE\\Putter et al. - 2020 - On the relation between the cause-specific hazard .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\4G6H5UWX\\bimj.html},
  journal = {Biometrical Journal},
  keywords = {cause-specific hazard,competing risks,cumulative incidence,proportional hazards,subdistribution hazard},
  language = {en},
  number = {3}
}

@article{putterTutorialBiostatisticsCompeting2007,
  title = {Tutorial in Biostatistics: Competing Risks and Multi-State Models},
  shorttitle = {Tutorial in Biostatistics},
  author = {Putter, H. and Fiocco, M. and Geskus, R. B.},
  year = {2007},
  volume = {26},
  pages = {2389--2430},
  issn = {1097-0258},
  abstract = {Standard survival data measure the time span from some time origin until the occurrence of one type of event. If several types of events occur, a model describing progression to each of these competing risks is needed. Multi-state models generalize competing risks models by also describing transitions to intermediate events. Methods to analyze such models have been developed over the last two decades. Fortunately, most of the analyzes can be performed within the standard statistical packages, but may require some extra effort with respect to data preparation and programming. This tutorial aims to review statistical methods for the analysis of competing risks and multi-state models. Although some conceptual issues are covered, the emphasis is on practical issues like data preparation, estimation of the effect of covariates, and estimation of cumulative incidence functions and state and transition probabilities. Examples of analysis with standard software are shown. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2712},
  copyright = {Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\YM2M2XYQ\\Putter et al. - 2007 - Tutorial in biostatistics competing risks and mul.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\FJNQWFT9\\sim.html},
  journal = {Statistics in Medicine},
  keywords = {competing risks,multi-state model,prediction,prognostic factors,survival analysis},
  language = {en},
  number = {11}
}

@article{quartagnoMultipleImputationDiscrete2019,
  title = {Multiple Imputation for Discrete Data: {{Evaluation}} of the Joint Latent Normal Model},
  shorttitle = {Multiple Imputation for Discrete Data},
  author = {Quartagno, Matteo and Carpenter, James R.},
  year = {2019},
  volume = {61},
  pages = {1003--1019},
  issn = {1521-4036},
  abstract = {Missing data are ubiquitous in clinical and social research, and multiple imputation (MI) is increasingly the methodology of choice for practitioners. Two principal strategies for imputation have been proposed in the literature: joint modelling multiple imputation (JM-MI) and full conditional specification multiple imputation (FCS-MI). While JM-MI is arguably a preferable approach, because it involves specification of an explicit imputation model, FCS-MI is pragmatically appealing, because of its flexibility in handling different types of variables. JM-MI has developed from the multivariate normal model, and latent normal variables have been proposed as a natural way to extend this model to handle categorical variables. In this article, we evaluate the latent normal model through an extensive simulation study and an application on data from the German Breast Cancer Study Group, comparing the results with FCS-MI. We divide our investigation in four sections, focusing on (i) binary, (ii) categorical, (iii) ordinal, and (iv) count data. Using data simulated from both the latent normal model and the general location model, we find that in all but one extreme general location model setting JM-MI works very well, and sometimes outperforms FCS-MI. We conclude the latent normal model, implemented in the R package jomo, can be used with confidence by researchers, both for single and multilevel multiple imputation.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201800222},
  copyright = {\textcopyright{} 2019 The Authors. Biometrical Journal Published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\DDAQIMN2\\Quartagno and Carpenter - 2019 - Multiple imputation for discrete data Evaluation .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\9JSKCQGG\\bimj.html},
  journal = {Biometrical Journal},
  keywords = {categorical data,joint model,latent normal model,missing data,multiple imputation},
  language = {en},
  number = {4}
}

@manual{rcoreteamLanguageEnvironmentStatistical2020,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}},
  type = {Manual}
}

@inproceedings{resche-rigonImputingMissingCovariate2012,
  title = {Imputing Missing Covariate Values in Presence of Competing Risk},
  booktitle = {International {{Society}} for {{Clinical Biostatistics Conference}}},
  author = {{Resche-Rigon}, Matthieu and White, Ian and Chevret, Sylvie},
  year = {2012},
  month = aug,
  address = {{Bergen, Norway, 19-23 August 2012, P22.10}},
  abstract = {Due to its flexibility, its practicability and its efficiency compared to the complete case analysis, multiple imputation by chained equations is widely used to impute missing data. Imputation models are built using regression models and it is well known that to avoid bias in the analysis model, the imputation model must include all the analysis model variables including the outcome. In survival analyses, outcome is defined by a binary event indicator D and the observed event or censoring time T. Unfortunately, estimates obtained by direct inclusion of D and T in the imputation model are biased. Using a Cox model, I. White and P. Royston showed that the imputation model should include the event indicator and the cumulative baseline hazard, and therefore recommended to include the Nelson-Aalen estimator. In the competing-risks setting, subjects may experiment one out of K distinct and exclusive events. Two main approaches have been proposed. The most common approach models the cause-specific hazard of the event of interest while the second approach models the sub-distribution hazard associated with the cumulative incidence. We propose to extend the work of I. White and P. Royston to the competing-risks setting by including in the imputation model the cumulative hazard of the competing events. Moreover, we will show that cumulative hazards of all the events that compete to each other should be included. Performance of our approach will be evaluated by a simulation study, then applied to a sample of 278 adult patients with acute myeloid leukaemia.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\9YJIXXCL\\Resche-Rigon - 2012 - .pdf}
}

@book{rubin:1987,
  title = {Multiple Imputation for Nonresponse in Surveys},
  author = {Rubin, D. B.},
  year = {1987},
  pages = {258},
  publisher = {{Wiley}},
  added-at = {2009-10-28T04:42:52.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/20a72dbca78fbd5bc8ba192f31f5d5f2f/jwbowers},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {ec0d6b5ac7fe288d46c91a4158ed0777},
  intrahash = {0a72dbca78fbd5bc8ba192f31f5d5f2f},
  keywords = {Bayesian Missing Sample approach data,survey},
  timestamp = {2009-10-28T04:43:19.000+0100}
}

@article{ruckerPresentingSimulationResults2014,
  title = {Presenting Simulation Results in a Nested Loop Plot},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2014},
  month = dec,
  volume = {14},
  pages = {129},
  issn = {1471-2288},
  abstract = {Statisticians investigate new methods in simulations to evaluate their properties for future real data applications. Results are often presented in a number of figures, e.g., Trellis plots. We had conducted a simulation study on six statistical methods for estimating the treatment effect in binary outcome meta-analyses, where selection bias (e.g., publication bias) was suspected because of apparent funnel plot asymmetry. We varied five simulation parameters: true treatment effect, extent of selection, event proportion in control group, heterogeneity parameter, and number of studies in meta-analysis. In combination, this yielded a total number of 768 scenarios. To present all results using Trellis plots, 12 figures were needed.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\D5DGZ26N\\R√ºcker and Schwarzer - 2014 - Presenting simulation results in a nested loop plo.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\M8X9MJGY\\1471-2288-14-129.html},
  journal = {BMC Medical Research Methodology},
  keywords = {Diagram,Graphical representation,Plot,Simulation study,Trellis plot},
  number = {1}
}

@article{scheteligLateTreatmentrelatedMortality2019,
  title = {Late Treatment-Related Mortality versus Competing Causes of Death after Allogeneic Transplantation for Myelodysplastic Syndromes and Secondary Acute Myeloid Leukemia},
  author = {Schetelig, Johannes and de Wreede, Liesbeth C. and van Gelder, Michel and Koster, Linda and Finke, J{\"u}rgen and Niederwieser, Dietger and Beelen, Dietrich and Mufti, G. J. and Platzbecker, Uwe and Ganser, Arnold and Heidenreich, Silke and Maertens, Johan and Soci{\'e}, Gerard and Brecht, Arne and Stelljes, Matthias and Kobbe, Guido and Volin, Liisa and Nagler, Arnon and Vitek, Antonin and Luft, Thomas and Ljungman, Per and {Yakoub-Agha}, Ibrahim and Robin, Marie and Kr{\"o}ger, Nicolaus},
  year = {2019},
  month = mar,
  volume = {33},
  pages = {686--695},
  publisher = {{Nature Publishing Group}},
  issn = {1476-5551},
  abstract = {The causes and rates of late patient-mortality following alloHCT for myelodysplastic syndromes or secondary acute myeloid leukemia were studied, to assess the contribution of relapse-related, treatment-related, and population factors. Data from EBMT on 6434 adults, who received a first alloHCT from January 2000 to December 2012, were retrospectively studied using combined land-marking, relative-survival methods and multi-state modeling techniques. Median age at alloHCT increased from 49 to 58 years, and the number of patients aged {$\geq$}65 years at alloHCT increased from 5 to 17\%. Overall survival probability was 53\% at 2 years and 35\% at 10 years post-alloHCT. Survival probability at 5 years from the 2-year landmark was 88\% for patients {$<$}45-year old and 63\% for patients {$\geq$}65-year old at alloHCT. Cumulative incidence of nonrelapse mortality (NRM) for patients {$<$}45-year old at transplant was 7\% rising to 25\% for patients aged {$\geq$}65. For older patients, 31\% of NRM-deaths could be attributed to population mortality. Favorable post-alloHCT long-term survival was seen; however, excess mortality-risk for all age groups was shown compared to the general population. A substantial part of total NRM for older patients was attributable to population mortality, information which aids the balanced explanation of post-HCT risk and helps improve long-term care.},
  copyright = {2018 The Author(s)},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\IKIUZ784\\Schetelig et al. - 2019 - Late treatment-related mortality versus competing .pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\9FJC7IJW\\s41375-018-0302-y.html},
  journal = {Leukemia},
  language = {en},
  number = {3}
}

@article{schoutenDanceMechanismsHow2018,
  title = {The {{Dance}} of the {{Mechanisms}}: {{How Observed Information Influences}} the {{Validity}} of {{Missingness Assumptions}}},
  shorttitle = {The {{Dance}} of the {{Mechanisms}}},
  author = {Schouten, Rianne Margaretha and Vink, Gerko},
  year = {2018},
  month = oct,
  pages = {0049124118799376},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  abstract = {Missing data in scientific research go hand in hand with assumptions about the nature of the missingness. When dealing with missing values, a set of beliefs has to be formulated about the extent to which the observed data may also hold for the missing parts of the data. It is vital that the validity of these missingness assumptions is verified, tested, and that assumptions are adjusted when necessary. In this article, we demonstrate how observed data structures could a priori indicate whether it is likely that our beliefs about the missingness can be trusted. To this end, we simulate complete data and generate missing values according several types of MCAR, MAR, and MNAR mechanisms. We demonstrate that in scenarios where the data correlations are either low or very substantial, strictly different mechanisms yield equivalent statistical inferences. In addition, we show that the choice of quantity of scientific interest together with the distribution of the nonresponse govern the validity of the missingness assumptions.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\5CIYDJE3\\Schouten and Vink - 2018 - The Dance of the Mechanisms How Observed Informat.pdf},
  journal = {Sociological Methods \& Research},
  keywords = {missing data methodology,missingness assumptions,multivariate amputation},
  language = {en}
}

@article{schoutenGeneratingMissingValues2018,
  title = {Generating Missing Values for Simulation Purposes: A Multivariate Amputation Procedure},
  shorttitle = {Generating Missing Values for Simulation Purposes},
  author = {Schouten, Rianne Margaretha and Lugtig, Peter and Vink, Gerko},
  year = {2018},
  month = oct,
  volume = {88},
  pages = {2909--2930},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  abstract = {Missing data form a ubiquitous problem in scientific research, especially since most statistical analyses require complete data. To evaluate the performance of methods dealing with missing data, researchers perform simulation studies. An important aspect of these studies is the generation of missing values in a simulated, complete data set: the amputation procedure. We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. The multivariate amputation procedure, on the other hand, generates reliable amputations and allows for a proper regulation of missing data problems. The procedure has additional features to generate any missing data scenario precisely as intended. Hence, the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.},
  annotation = {\_eprint: https://doi.org/10.1080/00949655.2018.1491577},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\YLUUJXTL\\Schouten et al. - 2018 - Generating missing values for simulation purposes.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\AU9BHS4B\\00949655.2018.html},
  journal = {Journal of Statistical Computation and Simulation},
  keywords = {evaluation,Missing data,multiple imputation,multivariate amputation},
  number = {15}
}

@article{shahComparisonRandomForest2014,
  title = {Comparison of {{Random Forest}} and {{Parametric Imputation Models}} for {{Imputing Missing Data Using MICE}}: {{A CALIBER Study}}},
  shorttitle = {Comparison of {{Random Forest}} and {{Parametric Imputation Models}} for {{Imputing Missing Data Using MICE}}},
  author = {Shah, Anoop D. and Bartlett, Jonathan W. and Carpenter, James and Nicholas, Owen and Hemingway, Harry},
  year = {2014},
  month = mar,
  volume = {179},
  pages = {764--774},
  issn = {0002-9262},
  abstract = {Multivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemiologic research. The ``true'' imputation model may contain nonlinearities which are not included in default imputation models. Random forest imputation is a machine learning technique which can accommodate nonlinearities and interactions and does not require a particular regression model to be specified. We compared parametric MICE with a random forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000 persons drawn from the 10,128 stable angina patients in the CALIBER database (Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; 2001\textendash 2010) with complete data on all covariates. Variables were artificially made ``missing at random,'' and the bias and efficiency of parameter estimates obtained using different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) hazard ratios, but random forest was more efficient and produced narrower confidence intervals. The second study used simulated data in which the partially observed variable depended on the fully observed variables in a nonlinear way. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was better. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets in which some patients have missing data.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\PGRKPATE\\Shah et al. - 2014 - Comparison of Random Forest and Parametric Imputat.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\RR96PB5K\\107562.html},
  journal = {American Journal of Epidemiology},
  number = {6}
}

@phdthesis{van2015multiple,
  title = {Multiple Imputation with Chained Equations and Survival Outcomes},
  author = {{van der Kruijk}, Monique},
  year = {2015},
  school = {Leiden University}
}

@article{vanbuurenFullyConditionalSpecification2006,
  title = {Fully Conditional Specification in Multivariate Imputation},
  author = {{van Buuren}, S. and Brand, J. P. L. and {Groothuis-Oudshoorn}, C. G. M. and Rubin, D. B.},
  year = {2006},
  month = dec,
  volume = {76},
  pages = {1049--1064},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
  annotation = {\_eprint: https://doi.org/10.1080/10629360600810434},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\GVHT3P97\\Buuren et al. - 2006 - Fully conditional specification in multivariate im.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\PQ6Q9M8T\\10629360600810434.html},
  journal = {Journal of Statistical Computation and Simulation},
  keywords = {Distributional compatibility,Gibbs sampling,Multiple imputation,Multivariate missing data,Proper imputation,Simulation},
  number = {12}
}

@techreport{vanbuurens.FlexibleMultivariateImputation1999,
  title = {Flexible {{Multivariate Imputation}} by {{MICE}}},
  author = {{van Buuren, S.} and {Oudshoorn, K.} and {TNO Preventie en Gezondheid}},
  year = {1999},
  month = jan,
  institution = {{TNO}},
  keywords = {Health,Historisch materiaal TNO},
  language = {en}
}

@article{vonhippelHowManyImputations2020,
  title = {How {{Many Imputations Do You Need}}? {{A Two}}-Stage {{Calculation Using}} a {{Quadratic Rule}}},
  shorttitle = {How {{Many Imputations Do You Need}}?},
  author = {{von Hippel}, Paul T.},
  year = {2020},
  month = aug,
  volume = {49},
  pages = {699--718},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  abstract = {When using multiple imputation, users often want to know how many imputations they need. An old answer is that 2\textendash 10 imputations usually suffice, but this recommendation only addresses the efficiency of point estimates. You may need more imputations if, in addition to efficient point estimates, you also want standard error (SE) estimates that would not change (much) if you imputed the data again. For replicable SE estimates, the required number of imputations increases quadratically with the fraction of missing information (not linearly, as previous studies have suggested). I recommend a two-stage procedure in which you conduct a pilot analysis using a small-to-moderate number of imputations, then use the results to calculate the number of imputations that are needed for a final analysis whose SE estimates will have the desired level of replicability. I implement the two-stage procedure using a new SAS macro called \%mi\_combine and a new Stata command called how\_many\_imputations.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\JJAVRMX3\\von Hippel - 2020 - How Many Imputations Do You Need A Two-stage Calc.pdf},
  journal = {Sociological Methods \& Research},
  keywords = {imputation,incomplete data,missing data,missing values,multiple imputation},
  language = {en},
  number = {3}
}

@article{whiteAvoidingBiasDue2010,
  title = {Avoiding Bias Due to Perfect Prediction in Multiple Imputation of Incomplete Categorical Variables},
  author = {White, Ian R. and Daniel, Rhian and Royston, Patrick},
  year = {2010},
  month = oct,
  volume = {54},
  pages = {2267--2275},
  issn = {0167-9473},
  abstract = {Multiple imputation is a popular way to handle missing data. Automated procedures are widely available in standard software. However, such automated procedures may hide many assumptions and possible difficulties from the view of the data analyst. Imputation procedures such as monotone imputation and imputation by chained equations often involve the fitting of a regression model for a categorical outcome. If perfect prediction occurs in such a model, then automated procedures may give severely biased results. This is a problem in some standard software, but it may be avoided by bootstrap methods, penalised regression methods, or a new augmentation procedure.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\M3T6WHJ4\\White et al. - 2010 - Avoiding bias due to perfect prediction in multipl.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\SFAR3EUR\\S0167947310001490.html},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {Missing data,Multiple imputation,Perfect prediction,Separation},
  language = {en},
  number = {10}
}

@article{whiteBiasEfficiencyMultiple2010,
  title = {Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values},
  author = {White, Ian R. and Carlin, John B.},
  year = {2010},
  volume = {29},
  pages = {2920--2931},
  issn = {1097-0258},
  abstract = {When missing data occur in one or more covariates in a regression model, multiple imputation (MI) is widely advocated as an improvement over complete-case analysis (CC). We use theoretical arguments and simulation studies to compare these methods with MI implemented under a missing at random assumption. When data are missing completely at random, both methods have negligible bias, and MI is more efficient than CC across a wide range of scenarios. For other missing data mechanisms, bias arises in one or both methods. In our simulation setting, CC is biased towards the null when data are missing at random. However, when missingness is independent of the outcome given the covariates, CC has negligible bias and MI is biased away from the null. With more general missing data mechanisms, bias tends to be smaller for MI than for CC. Since MI is not always better than CC for missing covariate problems, the choice of method should take into account what is known about the missing data mechanism in a particular substantive application. Importantly, the choice of method should not be based on comparison of standard errors. We propose new ways to understand empirical differences between MI and CC, which may provide insights into the appropriateness of the assumptions underlying each method, and we propose a new index for assessing the likely gain in precision from MI: the fraction of incomplete cases among the observed values of a covariate (FICO). Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3944},
  copyright = {Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\4IHI8M9U\\White and Carlin - 2010 - Bias and efficiency of multiple imputation compare.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\UB5FQGSU\\sim.html},
  journal = {Statistics in Medicine},
  keywords = {complete-case analysis,missing covariates,missing data,multiple imputation},
  language = {en},
  number = {28}
}

@article{whiteImputingMissingCovariate2009,
  title = {Imputing Missing Covariate Values for the {{Cox}} Model},
  author = {White, Ian R. and Royston, Patrick},
  year = {2009},
  volume = {28},
  pages = {1982--1998},
  issn = {1097-0258},
  abstract = {Multiple imputation is commonly used to impute missing data, and is typically more efficient than complete cases analysis in regression analysis when covariates have missing values. Imputation may be performed using a regression model for the incomplete covariates on other covariates and, importantly, on the outcome. With a survival outcome, it is a common practice to use the event indicator D and the log of the observed event or censoring time T in the imputation model, but the rationale is not clear. We assume that the survival outcome follows a proportional hazards model given covariates X and Z. We show that a suitable model for imputing binary or Normal X is a logistic or linear regression on the event indicator D, the cumulative baseline hazard H0(T), and the other covariates Z. This result is exact in the case of a single binary covariate; in other cases, it is approximately valid for small covariate effects and/or small cumulative incidence. If we do not know H0(T), we approximate it by the Nelson\textendash Aalen estimator of H(T) or estimate it by Cox regression. We compare the methods using simulation studies. We find that using logT biases covariate-outcome associations towards the null, while the new methods have lower bias. Overall, we recommend including the event indicator and the Nelson\textendash Aalen estimator of H(T) in the imputation model. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3618},
  copyright = {Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\CBXUZGJ7\\White and Royston - 2009 - Imputing missing covariate values for the Cox mode.pdf;C\:\\Users\\efbonneville\\Zotero\\storage\\3K63MTRJ\\sim.html},
  journal = {Statistics in Medicine},
  keywords = {survival},
  language = {en},
  number = {15}
}

@article{woodEstimationUsePredictions2015,
  title = {The Estimation and Use of Predictions for the Assessment of Model Performance Using Large Samples with Multiply Imputed Data},
  author = {Wood, Angela M. and Royston, Patrick and White, Ian R.},
  year = {2015},
  volume = {57},
  pages = {614--632},
  issn = {1521-4036},
  abstract = {Multiple imputation can be used as a tool in the process of constructing prediction models in medical and epidemiological studies with missing covariate values. Such models can be used to make predictions for model performance assessment, but the task is made more complicated by the multiple imputation structure. We summarize various predictions constructed from covariates, including multiply imputed covariates, and either the set of imputation-specific prediction model coefficients or the pooled prediction model coefficients. We further describe approaches for using the predictions to assess model performance. We distinguish between ideal model performance and pragmatic model performance, where the former refers to the model's performance in an ideal clinical setting where all individuals have fully observed predictors and the latter refers to the model's performance in a real-world clinical setting where some individuals have missing predictors. The approaches are compared through an extensive simulation study based on the UK700 trial. We determine that measures of ideal model performance can be estimated within imputed datasets and subsequently pooled to give an overall measure of model performance. Alternative methods to evaluate pragmatic model performance are required and we propose constructing predictions either from a second set of covariate imputations which make no use of observed outcomes, or from a set of partial prediction models constructed for each potential observed pattern of covariate. Pragmatic model performance is generally lower than ideal model performance. We focus on model performance within the derivation data, but describe how to extend all the methods to a validation dataset.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201400004},
  copyright = {\textcopyright{} 2015 The Author. Biometrical Journal published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  file = {C\:\\Users\\efbonneville\\Zotero\\storage\\PCX3YW66\\Wood et al. - 2015 - The estimation and use of predictions for the asse.pdf},
  journal = {Biometrical Journal},
  language = {en},
  number = {4}
}


